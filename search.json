[
  {
    "objectID": "syntax.html",
    "href": "syntax.html",
    "title": "Basics of representing syntax with VSAs",
    "section": "",
    "text": "In this section we will talk about the basics of specifying an abstract syntax, beginning first with our example language from the previous tutorial.\n\n\nPreviously, we discussed the small language \\(\\mathcal{L}\\) in VSAs. We were being a bit greedy with that symbol, so let us refer to the original language in the first example as \\(\\mathcal{L}_{\\text{fruit}}\\).\nWe also provided an example of defining a language inductively. It’s worthwhile to inexpect what we did and why we did it. Recall, the language \\(\\mathcal{L}_{\\text{fruit}}\\) is the minimal set of formulas which have the following properties:\n\nAtomic formulae: For atomic symbols \\(\\mathcal{A} = \\{\\texttt{apple}, \\texttt{banana},\n\\texttt{strawberry}\\}\\), if \\(x \\in \\mathcal{A}\\), then \\(x \\in \\mathcal{L}_\\text{fruit}\\).\n\nAtomic symbols like these are the most basic units of our language. E.g., if we are talking about the language of propositional logic, then the atomic values are the propositional variables \\(p, q, r, \\dots\\). But, when talking about VSAs, we think it to be best practice to talk about delicious treats.\n\nCompound formulae: If \\(\\phi \\in \\mathcal{L}_\\text{fruit}\\), and \\(\\psi \\in \\mathcal{L}_{\\text{fruit}}\\), then the tuple \\((\\phi, \\psi)\\) and the disjunction \\((\\phi \\lor \\psi)\\) are both in \\(\\mathcal{L}_\\text{fruit}\\).\n\nCompound statements are defined inductively over all the possible elements of \\(\\mathcal{L}_{\\text{fruit}}\\). This allows for arbitrary nesting tements. This is known as a compositional structure, roughly meaning that the things we’re talking about are made up of a small collection of basic units.\nSometimes definitions include a final case:\n\nAnything else that isn’t mentioned in the above conditions is not in \\(\\mathcal{L}_{\\text{fruit}}\\).\n\nBut, we have already covered this by mentioning that the set was the minimal set. Both of these prevent us from shoving in whatever we want into \\(\\mathcal{L}_\\text{fruit}\\).\n\n\n\nAnother way of defining an set like \\(\\mathcal{L}_\\text{fruit}\\) is to use something called Backus-Naur Form (BNF).\nUsing BNF for \\(\\mathcal{L}_\\text{fruit}\\):\n&lt;expr&gt; ::= &lt;atomic&gt;\n         | (&lt;expr&gt;, &lt;expr&gt;)\n         | (&lt;expr&gt; v &lt;expr&gt;)\n\n&lt;atomic&gt; ::= strawberry | apple | banana\nOn the left-hand side of the rule we have the name of the rule, in our case we have one for composite formulae called &lt;expr&gt;, and for atomic formulae we have &lt;atomic&gt;. On the right hand side of the rule we have either: a reference to another rule or a collection of symbols with mentions to other rules or raw symbols. For example, (&lt;expr&gt;, &lt;expr&gt;) says that the rule &lt;expr&gt; contains formulae that have the symbols (, ,, and ) as well as two &lt;expr&gt;’s inside of them.\nIn general, we can read BNF descriptions as inductive definitions like the section above:\n\nThe set called &lt;left-hand side name&gt; is the minimal set defined by the condition (1), (2), etc.\n\nFor more information about how to talk about sets of things defined inductively, Hopcroft and Ullman (1979) and Friemdan and Mitchell (2008).",
    "crumbs": [
      "Basics of representing syntax with VSAs"
    ]
  },
  {
    "objectID": "syntax.html#the-language-mathcall_textfruit",
    "href": "syntax.html#the-language-mathcall_textfruit",
    "title": "Basics of representing syntax with VSAs",
    "section": "",
    "text": "Previously, we discussed the small language \\(\\mathcal{L}\\) in VSAs. We were being a bit greedy with that symbol, so let us refer to the original language in the first example as \\(\\mathcal{L}_{\\text{fruit}}\\).\nWe also provided an example of defining a language inductively. It’s worthwhile to inexpect what we did and why we did it. Recall, the language \\(\\mathcal{L}_{\\text{fruit}}\\) is the minimal set of formulas which have the following properties:\n\nAtomic formulae: For atomic symbols \\(\\mathcal{A} = \\{\\texttt{apple}, \\texttt{banana},\n\\texttt{strawberry}\\}\\), if \\(x \\in \\mathcal{A}\\), then \\(x \\in \\mathcal{L}_\\text{fruit}\\).\n\nAtomic symbols like these are the most basic units of our language. E.g., if we are talking about the language of propositional logic, then the atomic values are the propositional variables \\(p, q, r, \\dots\\). But, when talking about VSAs, we think it to be best practice to talk about delicious treats.\n\nCompound formulae: If \\(\\phi \\in \\mathcal{L}_\\text{fruit}\\), and \\(\\psi \\in \\mathcal{L}_{\\text{fruit}}\\), then the tuple \\((\\phi, \\psi)\\) and the disjunction \\((\\phi \\lor \\psi)\\) are both in \\(\\mathcal{L}_\\text{fruit}\\).\n\nCompound statements are defined inductively over all the possible elements of \\(\\mathcal{L}_{\\text{fruit}}\\). This allows for arbitrary nesting tements. This is known as a compositional structure, roughly meaning that the things we’re talking about are made up of a small collection of basic units.\nSometimes definitions include a final case:\n\nAnything else that isn’t mentioned in the above conditions is not in \\(\\mathcal{L}_{\\text{fruit}}\\).\n\nBut, we have already covered this by mentioning that the set was the minimal set. Both of these prevent us from shoving in whatever we want into \\(\\mathcal{L}_\\text{fruit}\\).",
    "crumbs": [
      "Basics of representing syntax with VSAs"
    ]
  },
  {
    "objectID": "syntax.html#backus-naur-form",
    "href": "syntax.html#backus-naur-form",
    "title": "Basics of representing syntax with VSAs",
    "section": "",
    "text": "Another way of defining an set like \\(\\mathcal{L}_\\text{fruit}\\) is to use something called Backus-Naur Form (BNF).\nUsing BNF for \\(\\mathcal{L}_\\text{fruit}\\):\n&lt;expr&gt; ::= &lt;atomic&gt;\n         | (&lt;expr&gt;, &lt;expr&gt;)\n         | (&lt;expr&gt; v &lt;expr&gt;)\n\n&lt;atomic&gt; ::= strawberry | apple | banana\nOn the left-hand side of the rule we have the name of the rule, in our case we have one for composite formulae called &lt;expr&gt;, and for atomic formulae we have &lt;atomic&gt;. On the right hand side of the rule we have either: a reference to another rule or a collection of symbols with mentions to other rules or raw symbols. For example, (&lt;expr&gt;, &lt;expr&gt;) says that the rule &lt;expr&gt; contains formulae that have the symbols (, ,, and ) as well as two &lt;expr&gt;’s inside of them.\nIn general, we can read BNF descriptions as inductive definitions like the section above:\n\nThe set called &lt;left-hand side name&gt; is the minimal set defined by the condition (1), (2), etc.\n\nFor more information about how to talk about sets of things defined inductively, Hopcroft and Ullman (1979) and Friemdan and Mitchell (2008).",
    "crumbs": [
      "Basics of representing syntax with VSAs"
    ]
  },
  {
    "objectID": "syntax.html#code-example",
    "href": "syntax.html#code-example",
    "title": "Basics of representing syntax with VSAs",
    "section": "Code example",
    "text": "Code example\n\nclass Codebook(UserDict):\n    \"\"\"Thin dictionary wrapper for codebooks.\"\"\"\n\n    def __init__(self, symbols: list[str], dim: int) -&gt; None:\n        super(Codebook, self).__init__()\n        self.dim = dim\n        for symbol in symbols:\n            self.data[symbol] = random(1, dim).squeeze()\n\n\ndim = 400\ncodebook = Codebook(symbols=[\"like\", \"dislike\", \"apple\", \"banana\"], dim=dim)\n\n\ndef role_filler_pair(\n    struct: typing.Union[\n        dict[str, str], dict[str, HRR], dict[HRR, str], dict[HRR, HRR]\n    ],\n    codebook=codebook,\n) -&gt; HRR:\n    \"\"\"Create a role-filler pair from a dictionary.\"\"\"\n    v = np.zeros(codebook.dim).view(HRR)\n    for role, filler in struct.items():\n        if isinstance(role, str):\n            role = codebook[role]\n        if isinstance(filler, str):\n            filler = codebook[filler]\n        v += role.bind(filler)\n    v /= np.linalg.norm(v)\n    return v\n\n\n# Jane's Preference: likes: apple, dislikes: banana\njane_preference = role_filler_pair(\n    {\n        \"like\": \"apple\",\n        \"dislike\": \"banana\",\n    }\n)\n\n# Test whether or not Jane likes apples or bananas?\njane_like = jane_preference.bind(codebook[\"like\"].inverse())\nprint(f\"Jane likes apples: {jane_like.cosine_similarity(codebook['apple'])}\")\nprint(f\"Jane likes bananas: {jane_like.cosine_similarity(codebook['banana'])}\")\n\nJane likes apples: 0.5731136799350733\nJane likes bananas: 0.010662048594337383",
    "crumbs": [
      "Basics of representing syntax with VSAs"
    ]
  },
  {
    "objectID": "syntax.html#encoding",
    "href": "syntax.html#encoding",
    "title": "Basics of representing syntax with VSAs",
    "section": "Encoding",
    "text": "Encoding\nWe can now define the encoding function using this tagged-union approach:\n\ndim = 1_000\nT = [\"atomic\", \"tuple\", \"disjunction\"]\nR = [\"tag\", \"name\", \"lhs\", \"rhs\"]\nA = [\"strawberry\", \"banana\", \"apple\"]\ncodebook = Codebook(T + R + A, dim=dim)\n\n\ndef encode(expr: L, codebook: Codebook = codebook) -&gt; HRR:\n    \"\"\"Encode a formula in the language $\\mathcal{L}_{\\text{fruit}}$.\"\"\"\n    if not isinstance(expr, L):\n        raise TypeError(\"Expected a subclass of L\", expr)\n\n    if isinstance(expr, Atomic):\n        name = expr.name\n        return role_filler_pair(\n            {\n                \"tag\": \"atomic\",\n                \"name\": codebook[name],\n            },\n            codebook=codebook,\n        )\n    elif isinstance(expr, Tuple):\n        lhs = expr.lhs\n        rhs = expr.rhs\n        return role_filler_pair(\n            {\n                \"tag\": \"tuple\",\n                \"lhs\": encode(lhs, codebook=codebook),\n                \"rhs\": encode(rhs, codebook=codebook),\n            },\n            codebook=codebook,\n        )\n    elif isinstance(expr, Disjunction):\n        lhs = expr.lhs\n        rhs = expr.rhs\n        return role_filler_pair(\n            {\n                \"tag\": \"disjunction\",\n                \"lhs\": encode(lhs, codebook=codebook),\n                \"rhs\": encode(rhs, codebook=codebook),\n            },\n            codebook=codebook,\n        )\n\nWe can see below that the encoded representation for the atomic formula \\(\\texttt{apple}\\) is now structured. It loses the intuitive mapping from the previous encoding, with the tradeoff now that we can directly inspect the elements of the encoded representation without knowing already what is in the body.\n\napple = Atomic(\"apple\")\nenc_apple = encode(apple)\nis_atomic = enc_apple.bind(codebook[\"tag\"].inverse()).cosine_similarity(\n    codebook[\"atomic\"]\n)\nprint(f\"Is `enc_apple` atomic: {is_atomic:.2f}\")\nis_apple = enc_apple.bind(codebook[\"name\"].inverse()).cosine_similarity(\n    codebook[\"apple\"]\n)\nprint(f\"Is the name of `enc_apple` `apple: {is_apple:.2f}\")\n\nIs `enc_apple` atomic: 0.58\nIs the name of `enc_apple` `apple: 0.57\n\n\nIn fact, using this representation, we can actually decode the encoded formulae of the language.",
    "crumbs": [
      "Basics of representing syntax with VSAs"
    ]
  },
  {
    "objectID": "syntax.html#decoding",
    "href": "syntax.html#decoding",
    "title": "Basics of representing syntax with VSAs",
    "section": "Decoding",
    "text": "Decoding\nGiven that our representations are structured, and that we have the codebook of all potential values hanging around, we can decode the representations back into a human readable form (with some loss, but that’s the name of the game).\n\ndef decode(enc: HRR, codebook=codebook, theta: float = 0.2) -&gt; L:\n    \"\"\"Decode a representation back to ``L``.\"\"\"\n    tag = enc.bind(codebook[\"tag\"].inverse())\n    t_atom = codebook[\"atomic\"]\n    t_tuple = codebook[\"tuple\"]\n    t_disj = codebook[\"disjunction\"]\n\n    if tag.cosine_similarity(t_atom) &gt; theta:\n        name = enc.bind(codebook[\"name\"].inverse())\n\n        # Recall the name from the codebook\n        keys, values = zip(*codebook.items())\n        V = np.array(values)\n        sims = V @ name\n        argmax = np.argmax(sims)\n\n        return Atomic(keys[argmax])\n    else:\n        # Trick here is that both of the other representations have\n        # an `lhs` and a `rhs`.\n        lhs = enc.bind(codebook[\"lhs\"].inverse())\n        rhs = enc.bind(codebook[\"rhs\"].inverse())\n        dec_lhs = decode(lhs, codebook=codebook, theta=theta)\n        dec_rhs = decode(rhs, codebook=codebook, theta=theta)\n        if tag.cosine_similarity(t_tuple) &gt; theta:\n            return Tuple(dec_lhs, dec_rhs)\n        else:\n            return Disjunction(dec_lhs, dec_rhs)\n\n\n# Testing atomic:\nstraw = Atomic(\"strawberry\")\nprint(f\"Original form: {straw}\")\nenc_straw = encode(straw)\ndec_straw = decode(enc_straw)\nprint(f\"Decoded form: {dec_straw}\")\n\nprint()\n\n# Testing tuples:\napple = Atomic(\"apple\")\nenc_apple = encode(apple)\ntupl = Tuple(straw, apple)\nprint(f\"Original form: {tupl}\")\nenc_tupl = encode(tupl)\ndec_tupl = decode(enc_tupl)\nprint(f\"Decoded form: {dec_tupl}\")\n\nprint()\n\n# Testing disjunctions\ndisj = Disjunction(straw, apple)\nprint(f\"Original form: {disj}\")\nenc_disj = encode(disj)\ndec_disj = decode(enc_disj)\nprint(f\"Decoded form: {dec_disj}\")\n\nOriginal form: Atomic(name='strawberry')\nDecoded form: Atomic(name='strawberry')\n\nOriginal form: Tuple(lhs=Atomic(name='strawberry'), rhs=Atomic(name='apple'))\nDecoded form: Tuple(lhs=Atomic(name='strawberry'), rhs=Atomic(name='apple'))\n\nOriginal form: Disjunction(lhs=Atomic(name='strawberry'), rhs=Atomic(name='apple'))\nDecoded form: Disjunction(lhs=Atomic(name='strawberry'), rhs=Atomic(name='apple'))\n\n\nUnfortunately, decoding is brittle. We will talk about ways to remedy this in the next section.",
    "crumbs": [
      "Basics of representing syntax with VSAs"
    ]
  },
  {
    "objectID": "vsas.html",
    "href": "vsas.html",
    "title": "An Introduction to VSAs",
    "section": "",
    "text": "In this project we are going to be using Vector-Symbolic Architectures (VSAs), or, Hyperdimensional Computing (HDC) to encode the symbolic syntax of a programming language in high-dimensional vector space. But, what is a VSA? A VSA first provides a method for mapping symbols to high-dimensional vector. Suppose that we have a language \\(\\mathcal{L}\\) which has three atomic elements: \\(\\texttt{banana}\\), \\(\\texttt{strawberry}\\), and \\(\\texttt{apple}\\). The language might have some further ways for composing these representations, and we will investigate how to represent these later, but for right now we will focus only on the atomic elements of the language.\nThe first step for doing VSA computation is to create a mapping \\(f\\) which maps from the language \\(\\mathcal{L}\\) to the high-dimensional vector space \\(V^D\\). For simplicity, we will be considering only the vector space \\(\\mathbb{R}^D\\), but there are other VSAs which use different vector spaces. For atomic elements, the mapping \\(f\\) takes symbols in \\(\\mathcal{L}\\) to rows to the \\(N\\) rows (one for each atomic element) of \\(D\\)-dimension in the matrix \\(X\\) called the codebook: \\[\nX = [x_1, x_2, \\dots, x_N],\n\\tag{1}\n\\] and for some atomic \\(s \\in \\mathcal{L}\\), \\(f(s) = x_i\\).\n\nWe are being very informal about the definition of the syntax but later on we will be more formal by providing an actual inductive definition.\n\nNow that we have high-dimensional representations of each of the symbols, how can we compose them? First, let us extend \\(\\mathcal{L}\\) by adding the operator \\((\\cdot, \\cdot)\\), which creates a “tuple” of elements in \\(\\mathcal{L}\\). For example, the tuple of \\(\\texttt{banana}\\) and \\(\\texttt{apple}\\) is: \\[\nt = ( \\texttt{banana},~\\texttt{apple}).\n\\tag{2}\n\\]\nIntuitively, \\(t \\neq \\text{apple} \\neq \\text{banana}\\), even though \\(t\\) is composed of the atomic elements \\(\\texttt{apple}\\) and \\(\\texttt{banana}\\). Therefore, we need some function \\(d\\) defined over \\(X\\) such that for \\(x_i, x_j\\), \\[\nd (x_i, x_j) = \\begin{cases}\n    &1,~\\text{if}~f^{-1}(x_i) = f^{-1}(x_j), \\\\\n    &0,~\\text{otherwise}.\n\\end{cases}\n\\tag{3}\n\\] Given that we are in \\(\\mathbb{R}^D\\), define \\(d(x_i, x_j) = \\frac{x_i \\cdot x_j}{\\|x_i\\|\\|x_j\\|}\\), or the cosine similarity. This immediately gives us a function that is \\(1\\) iff \\(x_i\\) and \\(x_j\\) are identical, and interpolates to \\(0\\) as the vectors become more “dissimilar”.\nGiven that \\(D\\) is high-dimensional, the “blessing of dimensionality” means that each \\(x_i\\) and \\(x_j\\), \\(i \\neq j\\) are “pseudo-orthogonal”, meaning, it is more likely than not that they are dissimilar up to some constant \\(\\vartheta\\): \\[\nd(x_i, x_j) = 0 + \\vartheta.\n\\tag{4}\n\\] In order to encode the tuple function, we have to create a new representation out of the component elements which is dissimilar to the component elements. We can achieve this by an operation called bind, where the binding of elements \\(x_i\\) and \\(x_j\\), denoted by \\(x_i \\otimes x_j\\), is such that: \\[\n\\begin{align*}\nd(x_i \\otimes x_j, x_i) &= 0 + \\vartheta, \\\\\nd(x_i \\otimes x_j, x_j) &= 0 + \\vartheta.\n\\end{align*}\n\\tag{5}\n\\] Furthermore, bind must be invertible, where we have: \\[\n\\begin{align*}\n(x_i \\otimes x_j) \\otimes^{-1} x_i &= x_j + \\kappa, \\\\\n(x_i \\otimes x_j) \\otimes^{-1} x_j &= x_i + \\kappa,\n\\end{align*}\n\\tag{6}\n\\] with \\(\\kappa\\) being a noise term. This invertability is gives us the ability to extract component elements of the tuple.\nWith binding defined, we can encode \\(t\\) from (2): \\[\n\\begin{align*}\nf((\\texttt{banana},~\\texttt{apple})) &= f(\\texttt{banana}) \\otimes f(\\texttt{apple}), \\\\\n&= x_i \\otimes x_j.\n\\end{align*}\n\\tag{7}\n\\]\nFor the final key component, let’s add another operator \\((\\cdot \\lor \\cdot)\\), called disjunction. We can read this as “either …, or …” of other elements in \\(\\mathcal{L}\\). While tuples like \\(t\\) are completely dissimlar to their component elements, disjunctions can either be one of their elements or the other. For the sake of this model, we’ll say that we want the disjunction of element \\(s_1 \\in \\mathcal{L}\\) and element \\(s_2 \\in\n\\mathcal{L}\\) to be similar to components. In high-dimensional vector space, this can be encoded as superposition, where for two vectors \\(x_i\\) and \\(x_j\\), the superposition of \\(x_i\\) and \\(x_j\\), denoted \\(x_i \\oplus x_j\\) is such that: \\[\n\\begin{align*}\nd((x_i \\otimes x_i), x_i) &= 1 - \\vartheta , \\\\\nd((x_i \\otimes x_j), x_j) &= 1 - \\vartheta.\n\\end{align*}\n\\tag{8}\n\\] With this defined, we can define the mapping \\(f\\) over disjunctions as: \\[\n\\begin{align*}\nf((\\texttt{banana},~\\texttt{apple})) &= f(\\texttt{banana}) \\oplus f(\\texttt{apple}) \\\\\n&= x_i \\oplus x_j.\n\\end{align*}\n\\tag{9}\n\\] These three functions are the core of VSAs, and give us incredible expressive capacity.\n\n\nVSAs are a model of symbolic structures which are able to express compositional relationships between symbols as functions defined over high-dimensional vector space. The key three operations used to encode these symbolic relations are:\n\nSimilarity: similarity between two high-dimensional vectors in Eq’n. (3), which is close to \\(1\\) when they are “the same” and close to \\(0\\) when they aren’t.\nBinding: binding composes elements into a new, distinct symbol from their component parts. Binding satisfies the properties in Eq’n. (5).\nSuperposition: also called bundling (as a dual with binding), that makes a new high-dimensional vector which is similar to the component elements, satisfying the properties in Eq’n. (8).\n\nFor the language \\(\\mathcal{L}\\) defined as the minimal set which satisfies the following properties:\n\nAtomic elements \\(\\mathcal{A} = \\{\\texttt{apple},~\\texttt{banana},~\\texttt{strawberry}\\}\\) are in the set.\nIf \\(s \\in \\mathcal{L}\\) and \\(z \\in \\mathcal{L}\\), then the tuple \\((s, z)\\) is also in \\(\\mathcal{L}\\).\nIf \\(s \\in \\mathcal{L}\\) and \\(z \\in \\mathcal{L}\\), then the disjunction \\((s \\lor z)\\) is in \\(\\mathcal{L}\\).\nAnything not satisfying the above requirements is not in \\(\\mathcal{L}\\).\n\nWe can define the encoding function \\(f(\\mathcal{L}) = \\mathbb{R}^D\\) inductively:\n\nIf \\(x\\) is in \\(\\mathcal{A}\\), then \\(f(x) = x_i\\), for \\(D\\)-dimensional row vector in the codebook \\(X\\).\nIf \\(\\phi \\in \\mathcal{L}\\) and \\(\\psi \\in \\mathcal{L}\\), then \\[\nf((\\phi, \\psi)) = f(\\phi) \\otimes f(\\psi).\n\\]\nIf \\(\\phi \\in \\mathcal{L}\\) and \\(\\psi \\in \\mathcal{L}\\), then, \\[\nf ((\\phi \\lor \\psi)) = f(\\phi) \\oplus f(\\psi).\n\\]",
    "crumbs": [
      "An Introduction to VSAs"
    ]
  },
  {
    "objectID": "vsas.html#summary",
    "href": "vsas.html#summary",
    "title": "An Introduction to VSAs",
    "section": "",
    "text": "VSAs are a model of symbolic structures which are able to express compositional relationships between symbols as functions defined over high-dimensional vector space. The key three operations used to encode these symbolic relations are:\n\nSimilarity: similarity between two high-dimensional vectors in Eq’n. (3), which is close to \\(1\\) when they are “the same” and close to \\(0\\) when they aren’t.\nBinding: binding composes elements into a new, distinct symbol from their component parts. Binding satisfies the properties in Eq’n. (5).\nSuperposition: also called bundling (as a dual with binding), that makes a new high-dimensional vector which is similar to the component elements, satisfying the properties in Eq’n. (8).\n\nFor the language \\(\\mathcal{L}\\) defined as the minimal set which satisfies the following properties:\n\nAtomic elements \\(\\mathcal{A} = \\{\\texttt{apple},~\\texttt{banana},~\\texttt{strawberry}\\}\\) are in the set.\nIf \\(s \\in \\mathcal{L}\\) and \\(z \\in \\mathcal{L}\\), then the tuple \\((s, z)\\) is also in \\(\\mathcal{L}\\).\nIf \\(s \\in \\mathcal{L}\\) and \\(z \\in \\mathcal{L}\\), then the disjunction \\((s \\lor z)\\) is in \\(\\mathcal{L}\\).\nAnything not satisfying the above requirements is not in \\(\\mathcal{L}\\).\n\nWe can define the encoding function \\(f(\\mathcal{L}) = \\mathbb{R}^D\\) inductively:\n\nIf \\(x\\) is in \\(\\mathcal{A}\\), then \\(f(x) = x_i\\), for \\(D\\)-dimensional row vector in the codebook \\(X\\).\nIf \\(\\phi \\in \\mathcal{L}\\) and \\(\\psi \\in \\mathcal{L}\\), then \\[\nf((\\phi, \\psi)) = f(\\phi) \\otimes f(\\psi).\n\\]\nIf \\(\\phi \\in \\mathcal{L}\\) and \\(\\psi \\in \\mathcal{L}\\), then, \\[\nf ((\\phi \\lor \\psi)) = f(\\phi) \\oplus f(\\psi).\n\\]",
    "crumbs": [
      "An Introduction to VSAs"
    ]
  },
  {
    "objectID": "interpretation.html",
    "href": "interpretation.html",
    "title": "Basics of Interpretation",
    "section": "",
    "text": "In this section we will broadly discuss how to represent interpretation in programming languages encoded high-dimensional vector space. In order to work up to this point, first we will discuss some problems with the memory capacity of VSAs: specifically, how we can get around information-loss as a result of VSA operations by using associative memories. Then, we will discuss other ways of representing syntax. Finally, we will talk about what is interpretation and program execution by thinking about a way of talking about how programs work: denotational semantics. We will then apply this knowledge to another toy language, the LET language.",
    "crumbs": [
      "Basics of Interpretation"
    ]
  },
  {
    "objectID": "interpretation.html#creating-a-cleanup-and-associative-memory",
    "href": "interpretation.html#creating-a-cleanup-and-associative-memory",
    "title": "Basics of Interpretation",
    "section": "Creating a Cleanup and Associative Memory",
    "text": "Creating a Cleanup and Associative Memory\nFor more intuition here, we will be creating both a cleanup and associative memory. A cleanup memory \\(\\mathcal{C}\\) is an auto-associative memory which stores \\(N\\) high-dimensional vectors of dimension \\(D\\). If \\(x\\) is a high-dimensional vector stored in the weights of \\(\\mathcal C\\), then \\(\\mathcal C(x) \\approx x\\). If \\(x'\\) is a degraded form of a stored memory item in \\(C\\), then \\(\\mathcal C(x') \\approx x\\), where \\(x\\) is the original form of the memory item. Cleanup memories are useful for recovering the original form of bound variables.\nThe next associative memory that we will be using is a hetero-associative memory \\(\\mathcal D\\) that stores \\(N \\times N\\) items, or \\(N\\) addresses of high-dimensional vectors and \\(N\\) patterns of high-dimensional vectors. We write to memory both an address vector and a pattern vector. If \\(x\\) is a high-dimensional vector stored in the addresses in \\(\\mathcal D\\), and \\(y\\) is the a high-dimensional vector stored in the patterns of \\(\\mathcal D\\) associated with \\(x\\) (i.e., the row in both the pattern and address matrix is the same), then \\(\\mathcal D(x) \\approx y\\). If \\(x'\\) likewise is a degraded form of \\(x\\), and \\(y\\) the stored pattern for \\(x\\), then \\(\\mathcal D(x') \\approx y\\).\n\nclass CleanupMem:\n    \"\"\"Simple clean-up memory.\"\"\"\n\n    def __init__(self, dim: int, init_capacity: int = 20) -&gt; None:\n        # The dimensionality of the data\n        self.dim = dim\n        # The current capacity of the memory\n        self.capacity = init_capacity\n        # The number of stored traces\n        self.stored_traces = 0\n        # The weight matrix\n        self.W = np.zeros(shape=(init_capacity, dim))\n\n    def write(self, x: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Write a value to memory.\"\"\"\n        if self.stored_traces &gt;= self.capacity:\n            self.W = np.concatenate([self.W, np.zeros((self.capacity, self.dim))])\n            self.capacity *= 2\n        self.W[self.stored_traces, :] = x\n        self.stored_traces += 1\n        return x\n\n    def read(self, x: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Read a value from memory, returning the value and its recalled form\"\"\"\n        similarities = self.W @ x\n        max_sim_idx = np.argmax(np.abs(similarities))\n        recalled = self.W[max_sim_idx]\n        return x, recalled.view(HRR)\n\n    def __call__(self, x: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n        return self.read(x)\n\n\nclass AssocMem:\n    def __init__(self, dim: int, init_capacity: int = 20) -&gt; None:\n        self.dim = dim\n        self.capacity = init_capacity\n        self.stored_traces = 0\n        # addresses\n        self.A = np.zeros((self.capacity, self.dim))\n        # patterns\n        self.P = np.zeros((self.capacity, self.dim))\n\n    def write(self, x: np.ndarray, y: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Associate `(x, y)` in memory, returning the values.\"\"\"\n        if self.stored_traces &gt;= self.capacity:\n            self.A = np.concatenate([self.A, np.zeros((self.capacity, self.dim))])\n            self.P = np.concatenate([self.P, np.zeros((self.capacity, self.dim))])\n            self.capacity *= 2\n        self.A[self.stored_traces, :] = x\n        self.P[self.stored_traces, :] = y\n        self.stored_traces += 1\n        return x, y.view(HRR)\n\n    def read(self, x: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Read `x` from memory, returning the `x` and the resulting value.\"\"\"\n        similarities = self.A @ x\n        max_sim_idx = np.argmax(np.abs(similarities))\n        recalled_pattern = self.P[max_sim_idx]\n        return x, recalled_pattern.view(HRR)\n\n    def __call__(self, x: np.ndarray) -&gt; tuple[np.ndarray, np.ndarray]:\n        return self.read(x)\n\nLet’s see how these work. For the cleanup memory, we should be able to degrade a value and still be able to retrieve the original form.\n\ndim = 400\nX = random(10, dim)\ncleanup_mem = CleanupMem(dim=dim)\nfor x in X:\n    cleanup_mem.write(x)\n\n# Test regular recall\nx = X[1].squeeze()\n_, x_hat = cleanup_mem(x)\nprint(f\"Cosine sim between x and x_hat: {x.cosine_similarity(x_hat)}\")\n\nx_degraded = X[1].bind(X[0]).bind(X[2]).bind(X[0].inverse()).bind(X[2].inverse())\n_, x_deg_hat = cleanup_mem(x_degraded)\nprint(f\"Cosine sim between x and x_deg_hat: {x.cosine_similarity(x_deg_hat)}\")\n\nCosine sim between x and x_hat: 1.0\nCosine sim between x and x_deg_hat: 1.0\n\n\nLikewise, we should be able to recall arbitrary associated values even under degradation.\n\ndim = 400\nA = random(10, dim)\nP = random(10, dim)\nassoc = AssocMem(dim)\nfor a, p in zip(A, P):\n    assoc.write(a, p)\n\na = A[1]\np = P[1]\n_, p_hat = assoc(a)\nprint(\n    f\"Cosine sim between target pattern and recalled pattern: {p.cosine_similarity(p_hat)}\"\n)\n\na_degraded = A[1].bind(A[0]).bind(A[2]).bind(A[0].inverse()).bind(A[2].inverse())\n_, p_deg_hat = assoc(a_degraded)\nprint(\n    f\"Cosine similarity between target pattern and recalled pattern from degraded value: {p_deg_hat.cosine_similarity(p)}\"\n)\n\nCosine sim between target pattern and recalled pattern: 1.0000000000000002\nCosine similarity between target pattern and recalled pattern from degraded value: 1.0000000000000002",
    "crumbs": [
      "Basics of Interpretation"
    ]
  },
  {
    "objectID": "interpretation.html#applying-memories-to-encoding-and-decoding",
    "href": "interpretation.html#applying-memories-to-encoding-and-decoding",
    "title": "Basics of Interpretation",
    "section": "Applying memories to encoding and decoding",
    "text": "Applying memories to encoding and decoding\nNow that we have an associative memory and a cleanup memory, how do we apply it to decoding so that we can preserve information? Recall that role-filler pair encodings for syntactic forms has some degradation of information, especially whenever we have multiple binds and superpositions. Therefore, for each new role-filler pair that we create, what we will do is create a reference. We say that a fresh, random vector \\(p\\) is a reference for some high-dimensional vector \\(x\\) if \\(p\\) is arbitrary and unrelated with \\(x\\), and we associate \\(p\\) with \\(x\\) in associative memory.\nWe also can store the contents of role-filler pairs in cleanup memory. This helps further with the preservation of information in role-filler pairs. Figuring out when and when not to use these methods of indirection unfortunately has no real theory behind it. Rather, it is a practical decision made whenever we notice lots of information loss.\nFurthermore, instead of using explicit tag roles, what we will do is explicitly superpose the role-filler pair of the contents of the syntax with the tag itself. This let’s us do easier comparison, as well as further limits the possiblity of information loss.\n\ndef encode_with_references(\n    expr: L,\n    codebook: Codebook,\n    assoc_mem: AssocMem,\n    cleanup: CleanupMem,\n) -&gt; HRR:\n    \"\"\"Encode an expression with references and indirection.\"\"\"\n\n    if isinstance(expr, Atomic):\n        name = expr.name\n        rf = role_filler_pair(\n            {\n                \"name\": name,\n            },\n            codebook=codebook,\n        )\n        return rf + codebook[\"atomic\"]\n    elif isinstance(expr, Tuple):\n        lhs = expr.lhs\n        rhs = expr.rhs\n\n        enc_lhs = encode_with_references(\n            lhs, codebook=codebook, assoc_mem=assoc_mem, cleanup=cleanup\n        )\n        enc_rhs = encode_with_references(\n            rhs, codebook=codebook, assoc_mem=assoc_mem, cleanup=cleanup\n        )\n\n        cleanup.write(enc_lhs)\n        cleanup.write(enc_rhs)\n\n        rfpair = (\n            role_filler_pair(\n                {\"lhs\": enc_lhs, \"rhs\": enc_rhs},\n                codebook=codebook,\n            )\n            + codebook[\"tuple\"]\n        )\n        ptr = random(1, codebook.dim).squeeze()\n        assoc_mem.write(ptr, rfpair)\n        return ptr\n    elif isinstance(expr, Disjunction):\n        lhs = expr.lhs\n        rhs = expr.rhs\n\n        enc_lhs = encode_with_references(\n            lhs, codebook=codebook, assoc_mem=assoc_mem, cleanup=cleanup\n        )\n        enc_rhs = encode_with_references(\n            rhs, codebook=codebook, assoc_mem=assoc_mem, cleanup=cleanup\n        )\n\n        cleanup.write(enc_lhs)\n        cleanup.write(enc_rhs)\n\n        rfpair = (\n            role_filler_pair(\n                {\"lhs\": enc_lhs, \"rhs\": enc_rhs},\n                codebook=codebook,\n            )\n            + codebook[\"disjunction\"]\n        )\n        ptr = random(1, codebook.dim).squeeze()\n        assoc_mem.write(ptr, rfpair)\n        return ptr\n\n\ndim = 1_000\nT = [\"atomic\", \"tuple\", \"disjunction\"]\nR = [\"tag\", \"name\", \"lhs\", \"rhs\"]\nA = [\"strawberry\", \"banana\", \"apple\"]\ncodebook = Codebook(T + R + A, dim=dim)\n\ncleanup_mem = CleanupMem(dim=codebook.dim)\nfor item in codebook.values():\n    cleanup_mem.write(item)\nassoc_mem = AssocMem(dim=dim)\n\nstraw = Atomic(\"strawberry\")\nenc_straw = encode_with_references(\n    straw, codebook=codebook, assoc_mem=assoc_mem, cleanup=cleanup_mem\n)\nprint(\n    f\"Is the encoded `straw` atomic?: {enc_straw.cosine_similarity(codebook['atomic'])}\"\n)\n\nIs the encoded `straw` atomic?: 0.7015074268173801\n\n\n\n# Testing tuples\nbanana = Atomic(\"banana\")\nenc_banana = encode_with_references(\n    banana, codebook=codebook, assoc_mem=assoc_mem, cleanup=cleanup_mem\n)\n\nt = Tuple(straw, banana)\nptr_t = encode_with_references(\n    t, codebook=codebook, assoc_mem=assoc_mem, cleanup=cleanup_mem\n)\n\nprint(f\"Is the pointer a tuple?: {ptr_t.cosine_similarity(codebook['tuple'])}\")\n\n_, deref_t = assoc_mem.read(ptr_t)\n\nprint(\n    f\"Is the derefenced tuple a tuple?: {deref_t.cosine_similarity(codebook['tuple'])}\"\n)\nlhs = deref_t.bind(codebook[\"lhs\"].inverse())\n_, lhs = cleanup_mem.read(lhs)\nprint(f\"Similarity between lhs and atomic: {lhs.cosine_similarity(codebook['atomic'])}\")\nprint(\n    f\"Is the lhs a strawberry?: {lhs.bind(codebook['name'].inverse()).cosine_similarity(codebook['strawberry'])}\"\n)\n\nIs the pointer a tuple?: -0.012625734991703514\nIs the derefenced tuple a tuple?: 0.5754711851450405\nSimilarity between lhs and atomic: 0.7015074268173801\nIs the lhs a strawberry?: 0.5825174657553933\n\n\n\nDecoding\nNow that we’ve preserved the information more using indirection, we can also have an easier time in decoding compound representations.\n\nt = Tuple(straw, banana)\nt_nest = Tuple(t, t)\nptr_t = encode_with_references(\n    t_nest, codebook=codebook, assoc_mem=assoc_mem, cleanup=cleanup_mem\n)\n\n_, deref_t = assoc_mem.read(ptr_t)\n_, lhs_ptr = cleanup_mem.read(deref_t.bind(codebook[\"lhs\"].inverse()))\n_, deref_lhs = assoc_mem.read(lhs_ptr)\nprint(\n    f\"Is the left-hand side a tuple?: {deref_lhs.cosine_similarity(codebook['tuple'])}\"\n)\n\nIs the left-hand side a tuple?: 0.5754711851450405\n\n\nImplementing decoding the forms with indirection is left as an exercise for the reader. It is always important to remember: you can not be sure that a value that you have is an atomic value that isn’t the result of some kind of indirection. Therefore, one must always be wary of testing for whether the value is atomic. To do so, one can simply test whether some high-dimensional encoded vector is sufficiently similar to the atomic tag. Otherwise, you can treat it as a reference, and dereference it from memory.\n\ndef decode_with_references(\n    enc: HRR,\n    codebook: Codebook,\n    assoc_mem: AssocMem,\n    cleanup_mem: CleanupMem,\n) -&gt; L:\n    \"\"\"Decode from our novel encoding with references.\"\"\"\n    ...",
    "crumbs": [
      "Basics of Interpretation"
    ]
  },
  {
    "objectID": "interpretation.html#trees",
    "href": "interpretation.html#trees",
    "title": "Basics of Interpretation",
    "section": "Trees",
    "text": "Trees\nThe abstract syntax of a language can be represented as a tree using a data structure called a sparse coordinate tree (Soulos, et al., 2024). The representation uses two codebooks for content, the \\(C = [c_1, c_2, \\dots, c_N]\\) and position: \\(P = [p_1, p_2, \\dots, p_M]\\). Each \\(p_i\\), \\(i = 1, \\dots, M\\) denotes \\(M\\) possible coordinates within the tree. Soulos, et al. have some indexing information in the position vectors \\(p_i\\), but as long as they are sufficiently distinct they can be anything. Another possible option is to use the set of permutations of some base vector \\(p_1\\), and the other vectors as: \\[\n\\begin{align*}\np_2 &= \\mathcal{P}^1(p_1), \\\\\np_3 &= \\mathcal{P}^2(p_1), \\\\\n&\\vdots \\\\\np_m &= \\mathcal{P}^{(m-1)}(p_1),\n\\end{align*}\n\\tag{1}\n\\] where \\(\\mathcal{P}^n(\\cdot)\\) denotes the \\(n\\)-th permutation of the vector argument.\nPermutations can also be used to “protect” information. Recall that binding creates a new vector which is orthogonal to the operands of the binding. Therefore, if we bind each filler with \\(n\\) permutations of a role, then we create a new and highly uncorrelated vector that is uniquely recoverable by taking the inverse of the bind with the \\(n\\) permutation vectors.\nThis encoding scheme is intimately related with the tagged unions that tactic that we use above. Instead of using distinct roles for the attributes of the abstract syntax, instead they use an agnostic representation that represents the abstract syntax as tree.\nConsider the following tree: \nWith our tagged union representation, this is: \\[\nt_\\text{rf} = \\texttt{tuple} \\oplus (\n    (\\texttt{lhs} \\otimes (\\texttt{atomic} \\oplus (\\texttt{name} \\otimes \\texttt{strawberry}))) \\oplus\n    (\\texttt{rhs} \\otimes (\\texttt{atomic} \\oplus (\\texttt{name} \\otimes \\texttt{banana})))\n)\n\\tag{2}\n\\] For our positional encoding, let us assign the positions left-hand side first, so the top of the tree is \\(p_1\\), going down the left hand branch we have \\(p_2, p_3, p_4\\), and the right hand side \\(p_5, p_6, p_7\\). The representation would be: \\[\n\\begin{align*}\nt_\\text{pos} &= p_1 \\otimes \\texttt{tuple} \\oplus p_2 \\otimes \\texttt{atomic} \\oplus \\\\\n    &p_3 \\otimes \\texttt{name} \\oplus p_4 \\otimes \\texttt{strawberry} \\\\\n    &p_5 \\otimes \\texttt{atomic} \\oplus p_6 \\otimes \\texttt{name} \\\\\n    &p_7 \\otimes \\texttt{banana}.\n\\end{align*}\n\\tag{3}\n\\] This naive representation will of course be subject to problems of noise. But, there are smarter ways of doing this positional encoding. For example, another tree representation from Frady, et al. (2020) is to use permutations of a left and a right base vector.",
    "crumbs": [
      "Basics of Interpretation"
    ]
  },
  {
    "objectID": "interpretation.html#lists",
    "href": "interpretation.html#lists",
    "title": "Basics of Interpretation",
    "section": "Lists",
    "text": "Lists\nSimilar to the tree representation, we can use a sort of linked-list structure for encoding the syntax. In this approach, we simply have a codebook of structural roles: \\(\\texttt{left}\\), \\(\\texttt{right}\\), the empty list indicator \\(\\texttt{nil}\\), and a list marker \\(\\varphi\\), accompanied with a codebook of values that we wish to have as elements.\nEncoding atomic symbols remains a simple mapping from the symbol to the codebook. However, for non-atomic structures (composite expressions), we can concatenate the elements into a list by using the \\(\\mathsf{cons}\\) operation: \\[\n\\mathsf{cons} (x, y) = (\\texttt{left} \\otimes x) \\oplus (\\texttt{right} \\otimes y) \\oplus \\varphi.\n\\tag{4}\n\\] A singleton list is represented as \\(\\mathsf{cons}(x, \\texttt{nil})\\). This allows us to simply iterate through the list until by retrieving the \\(\\texttt{left}\\) item, performing our operation, and then treating the retrieved value from the \\(\\texttt{right}\\).\nLike our tagged-union representations these unfortunately are plagued with problems of noise. So, we must unfortunately enter \\(x\\) and \\(y\\) into cleanup memory to enhance recall. Given the ease of this representation, we will be using this for encoding abstract syntax. We’ll see an example below of how this works in practice. Before that, however, we must end our detour and talk about what programs mean: semantics.",
    "crumbs": [
      "Basics of Interpretation"
    ]
  },
  {
    "objectID": "interpretation.html#designing-a-denotation",
    "href": "interpretation.html#designing-a-denotation",
    "title": "Basics of Interpretation",
    "section": "Designing a denotation",
    "text": "Designing a denotation\nGiven a language \\(\\mathcal{L}\\), typically described by a list of syntactic roles, designing the semantics of the language requires us to define a function that, on the basis of the syntax of the language, assigns some meaning or denotation to the syntax.\nTo put it in simpler terms, we need to provide a function that maps from the syntax of \\(\\mathcal{L}\\) to some other domain that captures the essential properties of \\(\\mathcal{L}\\). We call this function the denotation function, denoted by \\(\\| \\cdot \\|\\).\n\nA denotation for \\(\\mathcal{L}_\\text{fruit}\\)\nTo provide an intuition here, suppose that \\(\\mathcal{L}_\\text{fruit}\\) is a programming language, and we want to design a semantics for this programming language. Of course, this language wouldn’t be very useful, except for describing collections of strawberries, bananas, and apples. But, it is good practice to begin to grasp what it takes to make a denotation.\nTo begin, we have to proceed recursively over the inductive definition of \\(\\mathcal{L}_\\text{fruit}\\). By defining the denotation in terms of atomic elements as a base case, and then composite elements, we are following something known as the principle of compositionality: namely, that the meaning of an expression in the language \\(\\mathcal{L}_\\text{let}\\) is determined the meaning of the parts of the expression.\nLet \\(\\mathcal{M}_\\text{fruit}\\) be a set called the model of \\(\\mathcal{L}_\\text{fruit}\\). The denotation function \\(\\| \\cdot \\|\\) maps from elements in \\(\\mathcal L_\\text{fruit}\\) to elements in \\(\\mathcal{M}_\\text{fruit}\\). To begin, we need to populate \\(\\mathcal{M}_\\text{fruit}\\). First, we need to have elements that correspond with the atomic elements of \\(\\mathcal{L}_\\text{fruit}\\). Let \\(\\mathsf{strawberry}\\), \\(\\mathsf{apple}\\), and \\(\\mathsf{banana}\\) be three elements in \\(\\mathcal{M}_\\text{fruit}\\). We can distinguish between elements in \\(\\mathcal{M}_\\text{fruit}\\) and atomic elements in \\(\\mathcal{L}_\\text{fruit}\\) by the lack of serifs. The denotation of atomic elements is thus: \\[\n\\begin{align*}\n\\| \\texttt{apple} \\| &= \\mathsf{apple}, \\\\\n\\| \\texttt{banana} \\| &= \\mathsf{banana}, \\\\\n\\| \\texttt{strawberry} \\| &= \\mathsf{strawberry}.\n\\end{align*}\n\\tag{5}\n\\]\nNext, we have to populate the composite expressions. For tuples, we will say that (1) the denotation of the left-hand side is in \\(\\mathcal{M}_\\text{fruit}\\), (2) the denotation of the right-hand side is in \\(\\mathcal{M}_\\text{fruit}\\), and (3) that the set of \\(\\{\\|\\text{lhs}\\|, \\{\\|\\text{lhs}\\|,~\\|\\text{rhs}\\|\\}\\}\\) is in \\(\\mathcal{M}_\\text{fruit}\\) (the set encoding of a tuple): \\[\n\\begin{align*}\n\\| (x, y) \\| = \\{ \\| x \\|, \\{ \\| x \\|, \\|y\\|\\}\\}.\n\\end{align*}\n\\tag{6}\n\\]",
    "crumbs": [
      "Basics of Interpretation"
    ]
  },
  {
    "objectID": "interpretation.html#real-world-example",
    "href": "interpretation.html#real-world-example",
    "title": "Basics of Interpretation",
    "section": "Real-world example",
    "text": "Real-world example\nA real-world example of how actual language specifications define the operational semantics of a language is the R5RS Standard for Scheme.",
    "crumbs": [
      "Basics of Interpretation"
    ]
  },
  {
    "objectID": "interpretation.html#the-semantics-for-mathcall_textlet",
    "href": "interpretation.html#the-semantics-for-mathcall_textlet",
    "title": "Basics of Interpretation",
    "section": "The Semantics for \\(\\mathcal{L}_\\text{let}\\)",
    "text": "The Semantics for \\(\\mathcal{L}_\\text{let}\\)\nThe semantics of \\(\\mathcal{L}_\\text{let}\\) require us to provide some function and set definitions. We will list them below, and then we will informally discuss what they are and what they do:\n\\[\n\\begin{align}\n\\text{Var} &= \\{x_1, x_2, \\dots\\} \\tag{7}  \\\\\n\\mathcal{n} &= \\{n_1, n_2, \\dots \\} \\tag{8} \\\\\n\\mathcal{A} &: \\mathbb{R}^D \\to \\mathbb{R}^D \\tag{9} \\\\\n\\mathcal{C} &: \\mathbb{R}^D \\to \\mathbb{R}^D \\tag{10} \\\\\n\\mathcal{X} &: \\text{Var} \\to \\mathbb{R}^D \\tag{11} \\\\\n\\mathcal{I} &: \\mathbb{N} \\to \\mathbb{R}^D \\tag{12} \\\\\n\\mathsf{checkInt} &: \\mathbb{R}^D \\to \\{0, 1\\}  \\tag{13} \\\\\n\\mathsf{expandToN} &: (\\mathcal{I} \\times \\mathbb{N}) \\to \\mathcal{I} \\tag {14}\\\\\n\\mathsf{diff} &: (\\mathbb{R}^D \\times \\mathbb{R}^D \\times \\mathcal{I}) \\to \\mathbb{R}^D \\tag{15} \\\\\n(\\cdot \\gets \\cdot) &: (\\mathcal{C} \\times \\mathbb{R}^D) \\to \\mathbb{0} \\tag{16} \\\\\n(\\cdot \\gets (\\cdot, \\cdot)) &: (\\mathcal{A} \\times \\mathbb{R}^D \\times \\mathbb{R}^D) \\to \\mathbb{0} \\tag{17} \\\\\n\\mathsf{updateCodebook} &: (\\mathcal{X} \\times \\text{Var}) \\to \\mathbb{0} \\tag{18} \\\\\n\\end{align}\n\\]",
    "crumbs": [
      "Basics of Interpretation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Compilation with VSAs.",
    "section": "",
    "text": "This is a primer for the thinking and intuitions behind encoding and interpreting programming languages in Vector-Symbolic Architectures (VSAs), based on Tomkins-Flanagan, et al. (2024) and Hanley, et al. (2025).\nWe will begin with a short introduction to VSAs, as well as how we can represent syntax with them. Then, we will do a discussion over programming language semantics, and converting these functions to work over high-dimensional vectors. We will then provide two concrete examples: an implementation of an interpreter for a small subset of LISP, as well as an implementation for a FORTH interpreter.\n\nReferences\n\nTomkins-Flanagan, E., & Kelly, M. A. (2024). Hey Pentti, We Did It!: A Fully Vector-Symbolic Lisp. International Conference on Cognitive Modeling.\n\n\nHanley, C., Tomkins-Flanagan, E., & Kelly, M. (2025). Hey Pentti, We Did (More of) It!: A Vector-Symbolic Lisp With Residue Arithmetic. International Joint Conference on Neural Networks.",
    "crumbs": [
      "Introduction to Compilation with VSAs."
    ]
  }
]