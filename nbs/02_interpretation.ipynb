{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec3e2f6",
   "metadata": {},
   "source": [
    "# Basics of Interpretation\n",
    "\n",
    "> Now that we have representations, what do we do with them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc38485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import typing\n",
    "from collections import UserDict\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from numpy.fft import fft, ifft\n",
    "\n",
    "\n",
    "class HRR(np.ndarray):\n",
    "    \"\"\"Thin wrapper around `np.ndarray` for Holographic Reduced Representations\n",
    "    (HRRs).\n",
    "    \"\"\"\n",
    "\n",
    "    # Incantation needed for subclassing `np.ndarray`.\n",
    "    def __new__(cls, input_array) -> object:\n",
    "        obj = np.asarray(input_array).view(cls)\n",
    "        return obj\n",
    "\n",
    "    # Same as above\n",
    "    def __array_finalize__(self, obj: object) -> None:\n",
    "        if obj is None:\n",
    "            return\n",
    "\n",
    "    # The binding operation.\n",
    "    def bind(self, other: typing.Union[np.ndarray, \"HRR\"]) -> \"HRR\":\n",
    "        \"\"\"Performcircular convolution.\n",
    "\n",
    "        Args:\n",
    "            other (np.ndarray): Second argument.\n",
    "\n",
    "        Returns:\n",
    "            Circular convolution of the vector and the other.\n",
    "        \"\"\"\n",
    "        v = ifft(fft(self) * fft(other)).real.view(HRR)\n",
    "        v /= np.linalg.norm(v)\n",
    "        return v\n",
    "\n",
    "    def inverse(self) -> \"HRR\":\n",
    "        \"\"\"Invert the vector for unbinding.\"\"\"\n",
    "        return self[np.r_[0, self.size - 1 : 0 : -1]].view(HRR)\n",
    "\n",
    "    def cosine_similarity(self, other: typing.Union[np.ndarray, \"HRR\"]) -> float:\n",
    "        norm = np.linalg.norm(self) * np.linalg.norm(other)\n",
    "        norm = max(norm, 1e-8)\n",
    "        return float((self.dot(other)) / norm)\n",
    "\n",
    "\n",
    "def random(\n",
    "    num_vectors: int,\n",
    "    dim: int,\n",
    "    dtype: npt.DTypeLike = float,\n",
    "    rng=np.random.default_rng(),\n",
    ") -> \"HRR\":\n",
    "    r\"\"\"Create matrix of `n` `d`-dimensional HRR vectors, sampled from the normal\n",
    "    distribution,\n",
    "    $$\n",
    "    \\mathcal{N}(\\mu=1, \\sigma^2 = \\frac{1}{d}),\n",
    "    $$\n",
    "\n",
    "    Args:\n",
    "        num_vectors int: The number of vector symbols you wish to generate.\n",
    "        dim int: The dimension of the vector symbols.\n",
    "        dtype npt.DTypeLike: The `dtype` of the vector generated. Default: ``float``.\n",
    "        rng: Random number generator.\n",
    "\n",
    "    Returns:\n",
    "        A ``(num_vectors, dim)`` matrix of random vector symbols.\n",
    "    \"\"\"\n",
    "    sd = 1.0 / np.sqrt(dim)\n",
    "    vs = rng.normal(scale=sd, size=(num_vectors, dim)).astype(dtype)\n",
    "    norms = np.linalg.vector_norm(vs, axis=1, keepdims=True)\n",
    "    vs /= norms\n",
    "    return HRR.__new__(cls=HRR, input_array=vs)\n",
    "\n",
    "\n",
    "class Codebook(UserDict):\n",
    "    \"\"\"Thin dictionary wrapper for codebooks.\"\"\"\n",
    "\n",
    "    def __init__(self, symbols: list[str], dim: int) -> None:\n",
    "        super(Codebook, self).__init__()\n",
    "        self.dim = dim\n",
    "        for symbol in symbols:\n",
    "            self.data[symbol] = random(1, dim).squeeze()\n",
    "\n",
    "\n",
    "def role_filler_pair(\n",
    "    struct: typing.Union[\n",
    "        dict[str, str], dict[str, HRR], dict[HRR, str], dict[HRR, HRR]\n",
    "    ],\n",
    "    codebook,\n",
    ") -> HRR:\n",
    "    \"\"\"Create a role-filler pair from a dictionary.\"\"\"\n",
    "    v = np.zeros(codebook.dim).view(HRR)\n",
    "    for role, filler in struct.items():\n",
    "        if isinstance(role, str):\n",
    "            role = codebook[role]\n",
    "        if isinstance(filler, str):\n",
    "            filler = codebook[filler]\n",
    "        v += role.bind(filler)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abf3162",
   "metadata": {},
   "source": [
    "# Overview of this Section\n",
    "\n",
    "In this section we will broadly discuss how to represent interpretation in \n",
    "programming languages encoded high-dimensional vector space. In order to\n",
    "work up to this point, first we will discuss some problems with the memory\n",
    "capacity of VSAs: specifically, how we can get around information-loss\n",
    "as a result of VSA operations by using *associative memories*. Then, we\n",
    "will discuss other ways of representing syntax. Finally, we will talk about\n",
    "what is interpretation and program execution by thinking about a way of \n",
    "talking about how programs work: [*denotational semantics*](https://en.wikipedia.org/wiki/Denotational_semantics).\n",
    "We will then apply this knowledge to another toy language, the LET language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c932ea4",
   "metadata": {},
   "source": [
    "# Associative Memories and Pointers\n",
    "\n",
    "Recall our previous example of encoding the simple language $\\mathcal{L}_\\text{fruit}$\n",
    "in high-dimensional vectors. We found that we can both *encode* the abstract\n",
    "syntax into the high-dimensional vectors using VSA operations, as well as \n",
    "decode them.\n",
    "\n",
    "But, as we will show, the decoding breaks down with even $1$-deep nesting of\n",
    "syntax. We say that a composite expression $\\phi$ is $n$-deep by counting the\n",
    "longest length of composite sub-expressions that it contains. Atomic expressions\n",
    "are $0$-deep.  Ideally, for languages which are compositional, we would like for any\n",
    "syntactic expression to be able to be arbitrarily deep. So, we could have\n",
    "a tuple of tuples of tuples of tuples, etc. But, because of the information\n",
    "loss inherent in VSA operations, we quickly find that without some indirection,\n",
    "this becomes untenable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14c803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original form: Tuple(lhs=Atomic(name='strawberry'), rhs=Atomic(name='apple'))\n",
      "Decoded form: Tuple(lhs=Atomic(name='strawberry'), rhs=Atomic(name='apple'))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from abc import ABCMeta\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class L(metaclass=ABCMeta):\n",
    "    \"\"\"Abstract base class of our language L.\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Atomic(L):\n",
    "    \"\"\"Abstract base class of atomic elements in the language.\"\"\"\n",
    "\n",
    "    name: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Tuple(L):\n",
    "    \"\"\"Tuples in L.\"\"\"\n",
    "\n",
    "    lhs: L\n",
    "    rhs: L\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Disjunction(L):\n",
    "    \"\"\"Disjunctions in L.\"\"\"\n",
    "\n",
    "    lhs: L\n",
    "    rhs: L\n",
    "\n",
    "\n",
    "dim = 1_000\n",
    "T = [\"atomic\", \"tuple\", \"disjunction\"]\n",
    "R = [\"tag\", \"name\", \"lhs\", \"rhs\"]\n",
    "A = [\"strawberry\", \"banana\", \"apple\"]\n",
    "codebook = Codebook(T + R + A, dim=dim)\n",
    "\n",
    "\n",
    "def encode(expr: L, codebook: Codebook = codebook) -> HRR:\n",
    "    \"\"\"Encode a formula in the language $\\mathcal{L}_{\\text{fruit}}$.\"\"\"\n",
    "    if not isinstance(expr, L):\n",
    "        raise TypeError(\"Expected a subclass of L\", expr)\n",
    "\n",
    "    if isinstance(expr, Atomic):\n",
    "        name = expr.name\n",
    "        return role_filler_pair(\n",
    "            {\n",
    "                \"tag\": \"atomic\",\n",
    "                \"name\": codebook[name],\n",
    "            },\n",
    "            codebook=codebook,\n",
    "        )\n",
    "    elif isinstance(expr, Tuple):\n",
    "        lhs = expr.lhs\n",
    "        rhs = expr.rhs\n",
    "        return role_filler_pair(\n",
    "            {\n",
    "                \"tag\": \"tuple\",\n",
    "                \"lhs\": encode(lhs, codebook=codebook),\n",
    "                \"rhs\": encode(rhs, codebook=codebook),\n",
    "            },\n",
    "            codebook=codebook,\n",
    "        )\n",
    "    elif isinstance(expr, Disjunction):\n",
    "        lhs = expr.lhs\n",
    "        rhs = expr.rhs\n",
    "        return role_filler_pair(\n",
    "            {\n",
    "                \"tag\": \"disjunction\",\n",
    "                \"lhs\": encode(lhs, codebook=codebook),\n",
    "                \"rhs\": encode(rhs, codebook=codebook),\n",
    "            },\n",
    "            codebook=codebook,\n",
    "        )\n",
    "\n",
    "\n",
    "def decode(enc: HRR, codebook=codebook, theta: float = 0.2) -> L:\n",
    "    \"\"\"Decode a representation back to ``L``.\"\"\"\n",
    "    tag = enc.bind(codebook[\"tag\"].inverse())\n",
    "    t_atom = codebook[\"atomic\"]\n",
    "    t_tuple = codebook[\"tuple\"]\n",
    "    t_disj = codebook[\"disjunction\"]\n",
    "\n",
    "    if tag.cosine_similarity(t_atom) > theta:\n",
    "        name = enc.bind(codebook[\"name\"].inverse())\n",
    "\n",
    "        # Recall the name from the codebook\n",
    "        keys, values = zip(*codebook.items())\n",
    "        V = np.array(values)\n",
    "        sims = V @ name\n",
    "        argmax = np.argmax(sims)\n",
    "\n",
    "        return Atomic(keys[argmax])\n",
    "    else:\n",
    "        # Trick here is that both of the other representations have\n",
    "        # an `lhs` and a `rhs`.\n",
    "        lhs = enc.bind(codebook[\"lhs\"].inverse())\n",
    "        rhs = enc.bind(codebook[\"rhs\"].inverse())\n",
    "        dec_lhs = decode(lhs, codebook=codebook, theta=theta)\n",
    "        dec_rhs = decode(rhs, codebook=codebook, theta=theta)\n",
    "        if tag.cosine_similarity(t_tuple) > theta:\n",
    "            return Tuple(dec_lhs, dec_rhs)\n",
    "        elif tag.cosine_similarity(t_disj) > theta:\n",
    "            return Disjunction(dec_lhs, dec_rhs)\n",
    "        else:\n",
    "            raise ValueError(\"Unkown value!\")\n",
    "\n",
    "\n",
    "# Testing tuples:\n",
    "straw = Atomic(\"strawberry\")\n",
    "apple = Atomic(\"apple\")\n",
    "enc_apple = encode(apple)\n",
    "tupl = Tuple(straw, apple)\n",
    "print(f\"Original form: {tupl}\")\n",
    "enc_tupl = encode(tupl)\n",
    "dec_tupl = decode(enc_tupl)\n",
    "print(f\"Decoded form: {dec_tupl}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Testing one-deep disjunctions\n",
    "# disj = Disjunction(tupl, apple)           # Uncommenting this throws errors\n",
    "# print(f\"Original form: {disj}\")\n",
    "# enc_disj = encode(disj)\n",
    "# dec_disj = decode(enc_disj)\n",
    "# print(f\"Decoded form: {dec_disj}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32633d6f",
   "metadata": {},
   "source": [
    "Even with a simple $1$-deep expression, our decoding function is unable to \n",
    "parse out that the left-hand side of `enc_disj` is a tuple. The key problem\n",
    "here is that we can think of VSA operations as a kind of information compression.\n",
    "Specifically, binding leads to a loss of information, and this loss\n",
    "depends on the kind of binding operation that we use (Kelly *et al.*, 2013).\n",
    "\n",
    "To solve this problem, we need to be able to add (1) indirection into the \n",
    "representation, allowing for the information to be preserved even under\n",
    "many binding operations, (2) a way to \"clean-up\" or \"clarify\" noisy representations,\n",
    "gravitating them back to their original form.\n",
    "\n",
    "To do this, we use *associative memories*, which are distributed, content-addressable\n",
    "memories that recontsruct queries based on stored information in their\n",
    "weights. Associative memories are well-suited for our high-dimensional representation\n",
    "as they are continuous and differentiable as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337cdaec",
   "metadata": {},
   "source": [
    "## Creating a Cleanup and Associative Memory\n",
    "\n",
    "For more intuition here, we will be creating both a cleanup and associative\n",
    "memory. A cleanup memory $\\mathcal{C}$ is an auto-associative memory which stores $N$\n",
    "high-dimensional vectors of dimension $D$. If $x$ is a high-dimensional\n",
    "vector stored in the weights of $\\mathcal C$, then $\\mathcal C(x) \\approx x$. If $x'$ is a \n",
    "degraded form of a stored memory item in $C$, then $\\mathcal C(x') \\approx x$,\n",
    "where $x$ is the original form of the memory item. Cleanup memories\n",
    "are useful for recovering the original form of bound variables.\n",
    "\n",
    "The next associative memory that we will be using is a hetero-associative\n",
    "memory $\\mathcal D$ that stores $N \\times N$ items, or $N$ addresses of high-dimensional\n",
    "vectors and $N$ patterns of high-dimensional vectors. We write to memory\n",
    "both an address vector and a pattern vector. If $x$ is a high-dimensional\n",
    "vector stored in the addresses in $\\mathcal D$, and $y$ is the a high-dimensional\n",
    "vector stored in the patterns of $\\mathcal D$ associated with $x$\n",
    "(i.e., the row in both the pattern and address matrix is the same), \n",
    "then $\\mathcal D(x) \\approx y$.  If $x'$ likewise is a degraded form of $x$, \n",
    "and $y$ the stored pattern for $x$, then $\\mathcal D(x') \\approx y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ff904",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanupMem:\n",
    "    \"\"\"Simple clean-up memory.\"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, init_capacity: int = 20) -> None:\n",
    "        # The dimensionality of the data\n",
    "        self.dim = dim\n",
    "        # The current capacity of the memory\n",
    "        self.capacity = init_capacity\n",
    "        # The number of stored traces\n",
    "        self.stored_traces = 0\n",
    "        # The weight matrix\n",
    "        self.W = np.zeros(shape=(init_capacity, dim))\n",
    "\n",
    "    def write(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Write a value to memory.\"\"\"\n",
    "        if self.stored_traces >= self.capacity:\n",
    "            self.W = np.concatenate([self.W, np.zeros((self.capacity, self.dim))])\n",
    "            self.capacity *= 2\n",
    "        self.W[self.stored_traces, :] = x\n",
    "        self.stored_traces += 1\n",
    "        return x\n",
    "\n",
    "    def read(self, x: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Read a value from memory, returning the value and its recalled form\"\"\"\n",
    "        similarities = self.W @ x\n",
    "        max_sim_idx = np.argmax(np.abs(similarities))\n",
    "        recalled = self.W[max_sim_idx]\n",
    "        return x, recalled.view(HRR)\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "        return self.read(x)\n",
    "\n",
    "\n",
    "class AssocMem:\n",
    "    def __init__(self, dim: int, init_capacity: int = 20) -> None:\n",
    "        self.dim = dim\n",
    "        self.capacity = init_capacity\n",
    "        self.stored_traces = 0\n",
    "        # addresses\n",
    "        self.A = np.zeros((self.capacity, self.dim))\n",
    "        # patterns\n",
    "        self.P = np.zeros((self.capacity, self.dim))\n",
    "\n",
    "    def write(self, x: np.ndarray, y: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Associate `(x, y)` in memory, returning the values.\"\"\"\n",
    "        if self.stored_traces >= self.capacity:\n",
    "            self.A = np.concatenate([self.A, np.zeros((self.capacity, self.dim))])\n",
    "            self.P = np.concatenate([self.P, np.zeros((self.capacity, self.dim))])\n",
    "            self.capacity *= 2\n",
    "        self.A[self.stored_traces, :] = x\n",
    "        self.P[self.stored_traces, :] = y\n",
    "        self.stored_traces += 1\n",
    "        return x, y.view(HRR)\n",
    "\n",
    "    def read(self, x: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Read `x` from memory, returning the `x` and the resulting value.\"\"\"\n",
    "        similarities = self.A @ x\n",
    "        max_sim_idx = np.argmax(np.abs(similarities))\n",
    "        recalled_pattern = self.P[max_sim_idx]\n",
    "        return x, recalled_pattern.view(HRR)\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "        return self.read(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e9b153",
   "metadata": {},
   "source": [
    "Let's see how these work. For the cleanup memory, we should be able to degrade\n",
    "a value and still be able to retrieve the original form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213d1c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine sim between x and x_hat: 1.0\n",
      "Cosine sim between x and x_deg_hat: 1.0\n"
     ]
    }
   ],
   "source": [
    "dim = 400\n",
    "X = random(10, dim)\n",
    "cleanup_mem = CleanupMem(dim=dim)\n",
    "for x in X:\n",
    "    cleanup_mem.write(x)\n",
    "\n",
    "# Test regular recall\n",
    "x = X[1].squeeze()\n",
    "_, x_hat = cleanup_mem(x)\n",
    "print(f\"Cosine sim between x and x_hat: {x.cosine_similarity(x_hat)}\")\n",
    "\n",
    "x_degraded = X[1].bind(X[0]).bind(X[2]).bind(X[0].inverse()).bind(X[2].inverse())\n",
    "_, x_deg_hat = cleanup_mem(x_degraded)\n",
    "print(f\"Cosine sim between x and x_deg_hat: {x.cosine_similarity(x_deg_hat)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6205d26",
   "metadata": {},
   "source": [
    "Likewise, we should be able to recall arbitrary associated values even under\n",
    "degradation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1b7157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine sim between target pattern and recalled pattern: 1.0000000000000002\n",
      "Cosine similarity between target pattern and recalled pattern from degraded value: 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "dim = 400\n",
    "A = random(10, dim)\n",
    "P = random(10, dim)\n",
    "assoc = AssocMem(dim)\n",
    "for a, p in zip(A, P):\n",
    "    assoc.write(a, p)\n",
    "\n",
    "a = A[1]\n",
    "p = P[1]\n",
    "_, p_hat = assoc(a)\n",
    "print(\n",
    "    f\"Cosine sim between target pattern and recalled pattern: {p.cosine_similarity(p_hat)}\"\n",
    ")\n",
    "\n",
    "a_degraded = A[1].bind(A[0]).bind(A[2]).bind(A[0].inverse()).bind(A[2].inverse())\n",
    "_, p_deg_hat = assoc(a_degraded)\n",
    "print(\n",
    "    f\"Cosine similarity between target pattern and recalled pattern from degraded value: {p_deg_hat.cosine_similarity(p)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d7b24c",
   "metadata": {},
   "source": [
    "## Applying memories to encoding and decoding\n",
    "\n",
    "Now that we have an associative memory and a cleanup memory, how do we apply\n",
    "it to decoding so that we can preserve information? Recall that role-filler\n",
    "pair encodings for syntactic forms has some degradation of information,\n",
    "especially whenever we have multiple binds and superpositions. Therefore,\n",
    "for each new role-filler pair that we create, what we will do is create\n",
    "a *reference*. We say that a fresh, random vector $p$ is a reference for some\n",
    "high-dimensional vector $x$ if $p$ is arbitrary and unrelated with $x$, and\n",
    "we associate $p$ with $x$ in associative memory.\n",
    "\n",
    "We also can store the contents of role-filler pairs in cleanup memory. This\n",
    "helps further with the preservation of information in role-filler pairs. \n",
    "Figuring out when and when not to use these methods of indirection \n",
    "unfortunately has no real theory behind it. Rather, it is a practical decision\n",
    "made whenever we notice lots of information loss.\n",
    "\n",
    "Furthermore, instead of using explicit tag roles, what we will do is \n",
    "explicitly superpose the role-filler pair of the contents of the syntax\n",
    "with the tag itself. This let's us do easier comparison, as well as further\n",
    "limits the possiblity of information loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2ca854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the encoded `straw` atomic?: 0.7218637832801331\n"
     ]
    }
   ],
   "source": [
    "def encode_with_references(\n",
    "    expr: L,\n",
    "    codebook: Codebook,\n",
    "    assoc_mem: AssocMem,\n",
    "    cleanup: CleanupMem,\n",
    ") -> HRR:\n",
    "    \"\"\"Encode an expression with references and indirection.\"\"\"\n",
    "\n",
    "    if isinstance(expr, Atomic):\n",
    "        name = expr.name\n",
    "        rf = role_filler_pair(\n",
    "            {\n",
    "                \"name\": name,\n",
    "            },\n",
    "            codebook=codebook,\n",
    "        )\n",
    "        return rf + codebook[\"atomic\"]\n",
    "    elif isinstance(expr, Tuple):\n",
    "        lhs = expr.lhs\n",
    "        rhs = expr.rhs\n",
    "\n",
    "        enc_lhs = encode_with_references(\n",
    "            lhs, codebook=codebook, assoc_mem=assoc_mem, cleanup=cleanup\n",
    "        )\n",
    "        enc_rhs = encode_with_references(\n",
    "            rhs, codebook=codebook, assoc_mem=assoc_mem, cleanup=cleanup\n",
    "        )\n",
    "\n",
    "        cleanup.write(enc_lhs)\n",
    "        cleanup.write(enc_rhs)\n",
    "\n",
    "        rfpair = (\n",
    "            role_filler_pair(\n",
    "                {\"lhs\": enc_lhs, \"rhs\": enc_rhs},\n",
    "                codebook=codebook,\n",
    "            )\n",
    "            + codebook[\"tuple\"]\n",
    "        )\n",
    "        ptr = random(1, codebook.dim).squeeze()\n",
    "        assoc_mem.write(ptr, rfpair)\n",
    "        return ptr\n",
    "    elif isinstance(expr, Disjunction):\n",
    "        lhs = expr.lhs\n",
    "        rhs = expr.rhs\n",
    "\n",
    "        enc_lhs = encode_with_references(\n",
    "            lhs, codebook=codebook, assoc_mem=assoc_mem, cleanup=cleanup\n",
    "        )\n",
    "        enc_rhs = encode_with_references(\n",
    "            rhs, codebook=codebook, assoc_mem=assoc_mem, cleanup=cleanup\n",
    "        )\n",
    "\n",
    "        cleanup.write(enc_lhs)\n",
    "        cleanup.write(enc_rhs)\n",
    "\n",
    "        rfpair = (\n",
    "            role_filler_pair(\n",
    "                {\"lhs\": enc_lhs, \"rhs\": enc_rhs},\n",
    "                codebook=codebook,\n",
    "            )\n",
    "            + codebook[\"disjunction\"]\n",
    "        )\n",
    "        ptr = random(1, codebook.dim).squeeze()\n",
    "        assoc_mem.write(ptr, rfpair)\n",
    "        return ptr\n",
    "\n",
    "\n",
    "dim = 1_000\n",
    "T = [\"atomic\", \"tuple\", \"disjunction\"]\n",
    "R = [\"tag\", \"name\", \"lhs\", \"rhs\"]\n",
    "A = [\"strawberry\", \"banana\", \"apple\"]\n",
    "codebook = Codebook(T + R + A, dim=dim)\n",
    "\n",
    "cleanup_mem = CleanupMem(dim=codebook.dim)\n",
    "for item in codebook.values():\n",
    "    cleanup_mem.write(item)\n",
    "assoc_mem = AssocMem(dim=dim)\n",
    "\n",
    "straw = Atomic(\"strawberry\")\n",
    "enc_straw = encode_with_references(\n",
    "    straw, codebook=codebook, assoc_mem=assoc_mem, cleanup=cleanup_mem\n",
    ")\n",
    "print(\n",
    "    f\"Is the encoded `straw` atomic?: {enc_straw.cosine_similarity(codebook['atomic'])}\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27741147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the pointer a tuple?: 0.007466406248505364\n",
      "Is the derefenced tuple a tuple?: 0.5889989642948561\n",
      "Similarity between lhs and atomic: 0.7218637832801331\n",
      "Is the lhs a strawberry?: 0.566608025894019\n"
     ]
    }
   ],
   "source": [
    "# Testing tuples\n",
    "banana = Atomic(\"banana\")\n",
    "enc_banana = encode_with_references(\n",
    "    banana, codebook=codebook, assoc_mem=assoc_mem, cleanup=cleanup_mem\n",
    ")\n",
    "\n",
    "t = Tuple(straw, banana)\n",
    "ptr_t = encode_with_references(\n",
    "    t, codebook=codebook, assoc_mem=assoc_mem, cleanup=cleanup_mem\n",
    ")\n",
    "\n",
    "print(f\"Is the pointer a tuple?: {ptr_t.cosine_similarity(codebook['tuple'])}\")\n",
    "\n",
    "_, deref_t = assoc_mem.read(ptr_t)\n",
    "\n",
    "print(f\"Is the derefenced tuple a tuple?: {deref_t.cosine_similarity(codebook['tuple'])}\")\n",
    "lhs = deref_t.bind(codebook[\"lhs\"].inverse())\n",
    "_, lhs = cleanup_mem.read(lhs)\n",
    "print(f\"Similarity between lhs and atomic: {lhs.cosine_similarity(codebook['atomic'])}\")\n",
    "print(f\"Is the lhs a strawberry?: {lhs.bind(codebook['name'].inverse()).cosine_similarity(codebook['strawberry'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1ff2ce",
   "metadata": {},
   "source": [
    "#### Decoding\n",
    "\n",
    "Now that we've preserved the information more using indirection, we can also\n",
    "have an easier time in decoding compound representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486f093d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the left-hand side a tuple?: 0.5889989642948561\n"
     ]
    }
   ],
   "source": [
    "t = Tuple(straw, banana)\n",
    "t_nest = Tuple(t, t)\n",
    "ptr_t = encode_with_references(\n",
    "    t_nest, codebook=codebook, assoc_mem=assoc_mem, cleanup=cleanup_mem\n",
    ")\n",
    "\n",
    "_, deref_t = assoc_mem.read(ptr_t)\n",
    "_, lhs_ptr = cleanup_mem.read(deref_t.bind(codebook[\"lhs\"].inverse()))\n",
    "_, deref_lhs = assoc_mem.read(lhs_ptr)\n",
    "print(f'Is the left-hand side a tuple?: {deref_lhs.cosine_similarity(codebook[\"tuple\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8c4e24",
   "metadata": {},
   "source": [
    "Implementing decoding the forms with indirection is left as an exercise for\n",
    "the reader. It is always important to remember: you can not be sure that a value\n",
    "that you have is an atomic value that isn't the result of some kind of indirection.\n",
    "Therefore, one must always be wary of testing for whether the value is atomic.\n",
    "To do so, one can simply test whether some high-dimensional encoded vector\n",
    "is sufficiently similar to the `atomic` tag. Otherwise, you can treat it as a\n",
    "reference, and dereference it from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cc7757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_with_references(\n",
    "    enc: HRR,\n",
    "    codebook: Codebook,\n",
    "    assoc_mem: AssocMem,\n",
    "    cleanup_mem: CleanupMem,\n",
    ") -> L:\n",
    "    \"\"\"Decode from our novel encoding with references.\"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a5fd72",
   "metadata": {},
   "source": [
    "# Alternative Representations of Syntax\n",
    "\n",
    "## Trees\n",
    "\n",
    "## Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6970ee",
   "metadata": {},
   "source": [
    "# Denotational Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5213b4",
   "metadata": {},
   "source": [
    "# The LET Programming Language\n",
    "\n",
    "The LET Programming language is a toy language that contains: simple\n",
    "expressions like addition and subtraction, testing whether or not an item\n",
    "is $0$, function application, conditional evaluation, and variable binding\n",
    "using `let` expressions.\n",
    "\n",
    "Let $\\mathcal{L}_{\\text{let}}$ be the language defined by the following BNF\n",
    "specification:\n",
    "```bnf\n",
    "<program> ::= <expression>\n",
    "\n",
    "<expression> ::= <number>\n",
    "              |  -(<expression>, <expression>)\n",
    "              | zero? (<expression>)\n",
    "              | if <expression> then <expression> else <expression>\n",
    "              | <identifier>\n",
    "              | let <expression> = <expression> in <expression>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17b414c",
   "metadata": {},
   "source": [
    "# Denotational Semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65677689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
