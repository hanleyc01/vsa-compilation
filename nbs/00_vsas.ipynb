{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to VSAs\n",
    "\n",
    "> Introduction to Vector-Symbolic Architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp vsas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "# |export\n",
    "import typing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from numpy.fft import fft, ifft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's a VSA\n",
    "\n",
    "In this project we are going to be using Vector-Symbolic Architectures (VSAs),\n",
    "or, Hyperdimensional Computing (HDC) to encode the symbolic syntax of a \n",
    "programming language in high-dimensional vector space. But, what is a VSA?\n",
    "A VSA first provides a method for mapping symbols to high-dimensional vector.\n",
    "Suppose that we have a language\n",
    "$\\mathcal{L}$ which has three atomic elements: $\\texttt{banana}$, \n",
    "$\\texttt{strawberry}$, and $\\texttt{apple}$. \n",
    "The language might have some further ways for composing these representations,\n",
    "and we will investigate how to represent these later, but for right now \n",
    "we will focus only on the atomic elements of the language.\n",
    "\n",
    "The first step for doing VSA computation is to create a mapping $f$ which maps\n",
    "from the language $\\mathcal{L}$ to the high-dimensional vector space $V^D$.\n",
    "For simplicity, we will be considering only the vector space $\\mathbb{R}^D$,\n",
    "but there are other VSAs which use different vector spaces. For atomic elements,\n",
    "the mapping $f$ takes symbols in $\\mathcal{L}$ to rows to the $N$ rows\n",
    "(one for each atomic element) of $D$-dimension in the matrix $X$ called the\n",
    "*codebook*:\n",
    "$$\n",
    "X = [x_1, x_2, \\dots, x_N],\n",
    "\\tag{1}\n",
    "$$\n",
    "and for some atomic $s \\in \\mathcal{L}$, $f(s) = x_i$.\n",
    "\n",
    "> We are being very informal about the definition of the syntax\n",
    "> but later on we will be more formal by providing an actual inductive definition.\n",
    "\n",
    "Now that we have high-dimensional representations of each of the symbols, how\n",
    "can we compose them? First, let us extend $\\mathcal{L}$ by adding the operator\n",
    "$(\\cdot, \\cdot)$, which creates a \"tuple\" of elements in $\\mathcal{L}$.\n",
    "For example, the tuple of $\\texttt{banana}$ and $\\texttt{apple}$ is:\n",
    "$$\n",
    "t = ( \\texttt{banana},~\\texttt{apple}).\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "Intuitively, $t \\neq \\text{apple} \\neq \\text{banana}$, even though $t$ is \n",
    "composed of the atomic elements $\\texttt{apple}$ and $\\texttt{banana}$.\n",
    "Therefore, we need some function $d$ defined over $X$ such that for $x_i, x_j$,\n",
    "$$\n",
    "d (x_i, x_j) = \\begin{cases}\n",
    "    &1,~\\text{if}~f^{-1}(x_i) = f^{-1}(x_j), \\\\\n",
    "    &0,~\\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\tag{3}\n",
    "$$\n",
    "Given that we are in $\\mathbb{R}^D$, define \n",
    "$d(x_i, x_j) = \\frac{x_i \\cdot x_j}{\\|x_i\\|\\|x_j\\|}$, or the *cosine similarity*.\n",
    "This immediately gives us a function that is $1$ iff $x_i$ and $x_j$ are identical,\n",
    "and interpolates to $0$ as the vectors become more \"dissimilar\". \n",
    "\n",
    "Given that $D$ is high-dimensional, the \"blessing of dimensionality\" means\n",
    "that each $x_i$ and $x_j$, $i \\neq j$ are \"pseudo-orthogonal\", meaning,\n",
    "it is more likely than not that they are dissimilar up to some constant $\\vartheta$:\n",
    "$$\n",
    "d(x_i, x_j) = 0 + \\vartheta.\n",
    "\\tag{4}\n",
    "$$\n",
    "In order to encode the tuple function, we have to create a new representation\n",
    "*out* of the component elements which is dissimilar to the component elements.\n",
    "We can achieve this by an operation called *bind*, where the *binding* \n",
    "of elements $x_i$ and $x_j$, denoted by $x_i \\otimes x_j$, is such that:\n",
    "$$\n",
    "\\begin{align*}\n",
    "d(x_i \\otimes x_j, x_i) &= 0 + \\vartheta, \\\\\n",
    "d(x_i \\otimes x_j, x_j) &= 0 + \\vartheta.\n",
    "\\end{align*}\n",
    "\\tag{5}\n",
    "$$\n",
    "Furthermore, *bind* must be invertible, where we have:\n",
    "$$\n",
    "\\begin{align*}\n",
    "(x_i \\otimes x_j) \\otimes^{-1} x_i &= x_j + \\kappa, \\\\\n",
    "(x_i \\otimes x_j) \\otimes^{-1} x_j &= x_i + \\kappa,\n",
    "\\end{align*}\n",
    "\\tag{6}\n",
    "$$\n",
    "with $\\kappa$ being a noise term. This invertability is gives us the ability\n",
    "to extract component elements of the tuple.\n",
    "\n",
    "With binding defined, we can encode $t$ from (2):\n",
    "$$\n",
    "\\begin{align*}\n",
    "f((\\texttt{banana},~\\texttt{apple})) &= f(\\texttt{banana}) \\otimes f(\\texttt{apple}), \\\\\n",
    "&= x_i \\otimes x_j.\n",
    "\\end{align*}\n",
    "\\tag{7}\n",
    "$$\n",
    "\n",
    "For the final key component, let's add another operator $(\\cdot \\lor \\cdot)$,\n",
    "called *disjunction*. We can read this as \"either ..., or ...\" of other\n",
    "elements in $\\mathcal{L}$. While tuples like $t$ are completely dissimlar\n",
    "to their component elements, disjunctions can either be one of their\n",
    "elements or the other. For the sake of this model, we'll say that we\n",
    "want the disjunction of element $s_1 \\in \\mathcal{L}$ and element $s_2 \\in\n",
    "\\mathcal{L}$ to be similar to components. In high-dimensional vector space,\n",
    "this can be encoded as *superposition*, where for two vectors $x_i$ and $x_j$,\n",
    "the *superposition* of $x_i$ and $x_j$, denoted $x_i \\oplus x_j$ is such that:\n",
    "$$\n",
    "\\begin{align*}\n",
    "d((x_i \\otimes x_i), x_i) &= 1 - \\vartheta , \\\\\n",
    "d((x_i \\otimes x_j), x_j) &= 1 - \\vartheta.\n",
    "\\end{align*}\n",
    "\\tag{8}\n",
    "$$\n",
    "With this defined, we can define the mapping $f$ over disjunctions as:\n",
    "$$\n",
    "\\begin{align*}\n",
    "f((\\texttt{banana},~\\texttt{apple})) &= f(\\texttt{banana}) \\oplus f(\\texttt{apple}) \\\\\n",
    "&= x_i \\oplus x_j.\n",
    "\\end{align*}\n",
    "\\tag{9}\n",
    "$$\n",
    "These three functions are the core of VSAs, and give us incredible expressive\n",
    "capacity.\n",
    "\n",
    "## Summary\n",
    "\n",
    "VSAs are a model of symbolic structures which are able to express compositional\n",
    "relationships between symbols as functions defined over high-dimensional vector\n",
    "space. The key three operations used to encode these symbolic relations are:\n",
    "\n",
    "1. **Similarity**: similarity between two high-dimensional vectors in Eq'n. (3), which\n",
    "is close to $1$ when they are \"the same\" and close to $0$ when they aren't.\n",
    "\n",
    "2. **Binding**: binding composes elements into a new, distinct symbol from their\n",
    "component parts. Binding satisfies the properties in Eq'n. (5).\n",
    "\n",
    "3. **Superposition**: also called *bundling* (as a dual with binding), that makes\n",
    "a new high-dimensional vector which is similar to the component elements,\n",
    "satisfying the properties in Eq'n. (8).\n",
    "\n",
    "For the language $\\mathcal{L}$ defined as the minimal set which satisfies\n",
    "the following properties:\n",
    "\n",
    "1. Atomic elements $\\mathcal{A} = \\{\\texttt{apple},~\\texttt{banana},~\\texttt{strawberry}\\}$ \n",
    "are in the set.\n",
    "\n",
    "2. If $s \\in \\mathcal{L}$ and $z \\in \\mathcal{L}$, then the *tuple* $(s, z)$\n",
    "is also in $\\mathcal{L}$.\n",
    "\n",
    "3. If $s \\in \\mathcal{L}$ and $z \\in \\mathcal{L}$, then the *disjunction*\n",
    "$(s \\lor z)$ is in $\\mathcal{L}$.\n",
    "\n",
    "4. Anything not satisfying the above requirements is not in $\\mathcal{L}$.\n",
    "\n",
    "We can define the encoding function $f(\\mathcal{L}) = \\mathbb{R}^D$ inductively:\n",
    "\n",
    "1. If $x$ is in $\\mathcal{A}$, then $f(x) = x_i$, for $D$-dimensional row\n",
    "vector in the codebook $X$.\n",
    "\n",
    "2. If $\\phi \\in \\mathcal{L}$ and $\\psi \\in \\mathcal{L}$, then \n",
    "$$\n",
    "f((\\phi, \\psi)) = f(\\phi) \\otimes f(\\psi).\n",
    "$$\n",
    "\n",
    "3. If $\\phi \\in \\mathcal{L}$ and $\\psi \\in \\mathcal{L}$, then,\n",
    "$$\n",
    "f ((\\phi \\lor \\psi)) = f(\\phi) \\oplus f(\\psi).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holographic Reduced Representations.\n",
    "\n",
    "Holographic Reduced Representations (HRRs) will our VSA of choice for this\n",
    "project [(Plate, 1995)](https://pages.ucsd.edu/~msereno/170/readings/06-Holographic.pdf).\n",
    "\n",
    "HRRs use high-dimensional vectors in real space. For the binding operator,\n",
    "we use circular convolution. Because circular convolution is component-wise\n",
    "multiplication in Fourier space, we implement it using the simple:\n",
    "$$\n",
    "x \\otimes y = \\mathfrak{R}\\left\\{\n",
    "    \\mathcal{F^{-1}}\\left[ \n",
    "        \\mathcal{F}(x) \\mathcal{F}(y)\n",
    "        \\right]\n",
    "\\right\\},\n",
    "\\tag{10}\n",
    "$$\n",
    "with $\\mathfrak{R}$ denoting the *real* components of the vector, discarding the\n",
    "complex components; $\\mathcal{F}$ the Fourier transform, and $\\mathcal{F}^{-1}$\n",
    "the *inverse* Fourier transform.\n",
    "\n",
    "For the similarity measure we use cosine similarity, and for superposition\n",
    "we use component-wise addition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding it up!\n",
    "\n",
    "The definition above was a little bit abstract, but it should become clearer\n",
    "once we have a coding implementation in our hands. We're going to use \n",
    "`numpy` as our numerical processing library, though we could do any other\n",
    "(if we like)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HRR(np.ndarray):\n",
    "    \"\"\"Thin wrapper around `np.ndarray` for Holographic Reduced Representations\n",
    "    (HRRs).\n",
    "    \"\"\"\n",
    "\n",
    "    # Incantation needed for subclassing `np.ndarray`.\n",
    "    def __new__(cls, input_array) -> object:\n",
    "        obj = np.asarray(input_array).view(cls)\n",
    "        return obj\n",
    "\n",
    "    # Same as above\n",
    "    def __array_finalize__(self, obj: object) -> None:\n",
    "        if obj is None:\n",
    "            return\n",
    "\n",
    "    # The binding operation.\n",
    "    def bind(self, other: typing.Union[np.ndarray, \"HRR\"]) -> \"HRR\":\n",
    "        \"\"\"Perform circular convolution.\n",
    "\n",
    "        Args:\n",
    "            other (np.ndarray): Second argument.\n",
    "\n",
    "        Returns:\n",
    "            Circular convolution of the vector and the other.\n",
    "        \"\"\"\n",
    "        return ifft(fft(self) * fft(other)).real.view(HRR)\n",
    "\n",
    "    def inverse(self) -> \"HRR\":\n",
    "        \"\"\"Invert the vector for unbinding.\"\"\"\n",
    "        return self[np.r_[0, self.size - 1 : 0 : -1]].view(HRR)\n",
    "\n",
    "    def cosine_similarity(self, other: typing.Union[np.ndarray, \"HRR\"]) -> float:\n",
    "        return float((self.dot(other)) / (np.linalg.norm(self) * np.linalg.norm(other)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a way of generating new vectors. This function helps us create\n",
    "arbitrary new symbols for each atomic symbol in the language $\\mathcal{L}$. To\n",
    "do this, we sample elements from the normal distribution:\n",
    "$$\n",
    "\\mathcal{N}\\left(\\mu=1, \\sigma = \\frac{1}{d}\\right),\n",
    "$$\n",
    "and then normalize them to unit magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random(\n",
    "    num_vectors: int,\n",
    "    dim: int,\n",
    "    dtype: npt.DTypeLike = float,\n",
    "    rng=np.random.default_rng(),\n",
    ") -> \"HRR\":\n",
    "    r\"\"\"Create matrix of `n` `d`-dimensional HRR vectors, sampled from the normal\n",
    "    distribution,\n",
    "    $$\n",
    "    \\mathcal{N}(\\mu=1, \\sigma^2 = \\frac{1}{d}),\n",
    "    $$\n",
    "\n",
    "    Args:\n",
    "        num_vectors int: The number of vector symbols you wish to generate.\n",
    "        dim int: The dimension of the vector symbols.\n",
    "        dtype npt.DTypeLike: The `dtype` of the vector generated. Default: ``float``.\n",
    "        rng: Random number generator.\n",
    "\n",
    "    Returns:\n",
    "        A ``(num_vectors, dim)`` matrix of random vector symbols.\n",
    "    \"\"\"\n",
    "    sd = 1.0 / np.sqrt(dim)\n",
    "    vs = rng.normal(scale=sd, size=(num_vectors, dim)).astype(dtype)\n",
    "    norms = np.linalg.vector_norm(vs, axis=1, keepdims=True)\n",
    "    vs /= norms\n",
    "    return HRR.__new__(cls=HRR, input_array=vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect some randomly generated vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = random(2, 28 * 28)\n",
    "\n",
    "\n",
    "def display_im(x, title=None):\n",
    "    plt.imshow(x.reshape(28, 28))\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "display_im(X[0], title=\"Randomly Generated Normal Vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also test our important measures: similarity, binding, and superposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between x_0 and x_1: -0.004668307104146607\n",
      "Cosine similarity between x_0 and (x_0 * x_1): 0.1232672204723025\n",
      "Cosine similarity between (t / x_1) and x_0: 0.6961395290534296\n",
      "Cosine similarity between x_0 and (x_0 + x_1): 0.7054543546168858\n"
     ]
    }
   ],
   "source": [
    "x_0 = X[0]\n",
    "x_1 = X[1]\n",
    "\n",
    "print(f\"Cosine similarity between x_0 and x_1: {x_0.cosine_similarity(x_1)}\")\n",
    "\n",
    "t = x_0.bind(x_1)\n",
    "print(f\"Cosine similarity between x_0 and (x_0 * x_1): {x_0.cosine_similarity(t)}\")\n",
    "\n",
    "unbound = t.bind(x_1.inverse())\n",
    "print(f\"Cosine similarity between (t / x_1) and x_0: {unbound.cosine_similarity(x_0)}\")\n",
    "\n",
    "spose = x_0 + x_1\n",
    "print(f\"Cosine similarity between x_0 and (x_0 + x_1): {spose.cosine_similarity(x_0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Abstract Syntax of $\\mathcal{L}$.\n",
    "\n",
    "Now that we have the basics of VSAs. In fashion that will become more typical,\n",
    "we will define the *abstract syntax* of our language $\\mathcal{L}$. Recall,\n",
    "$\\mathcal{L}$ has three basic parts: the atomic elements $\\mathcal{A}$,\n",
    "tuples, and disjunctions.\n",
    "\n",
    "We will define a representation for each of these, and then define\n",
    "an *encoding* function $f$ that maps these representations onto HRRs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from abc import ABCMeta\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class L(metaclass=ABCMeta):\n",
    "    \"\"\"Abstract base class of our language L.\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Atomic(L):\n",
    "    \"\"\"Abstract base class of atomic elements in the language.\"\"\"\n",
    "\n",
    "    name: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Tuple(L):\n",
    "    \"\"\"Tuples in L.\"\"\"\n",
    "\n",
    "    lhs: L\n",
    "    rhs: L\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Disjunction(L):\n",
    "    \"\"\"Disjunctions in L.\"\"\"\n",
    "\n",
    "    lhs: L\n",
    "    rhs: L\n",
    "\n",
    "\n",
    "def create_codebook(atomic_symbols: list[str], dim: int) -> HRR:\n",
    "    \"\"\"Create a codebook of atomic symbols.\n",
    "\n",
    "    Args:\n",
    "        atomic_symbols list[str]: The list of atomic symbols in the language.\n",
    "        dim int: The dimension of the vector symbols corresponding to the atomic symbols.\n",
    "\n",
    "    Returns:\n",
    "        A ``(len(atomic_symbols), dim)`` matrix codebook.\n",
    "    \"\"\"\n",
    "    N, D = len(atomic_symbols), dim\n",
    "    return random(N, D)\n",
    "\n",
    "\n",
    "def encode(expression: L, atomic_symbols: list[str], codebook: HRR) -> HRR:\n",
    "    r\"\"\"Encode an expression in $\\mathcal{L}$ to HRR.\n",
    "\n",
    "    Args:\n",
    "        expression L: The expression to encode.\n",
    "        atomic_symbols list[str]: The list of atomic symbols.\n",
    "        codebook HRR: ``(len(atomic_symbols), D)`` matrix codebook.\n",
    "\n",
    "    Returns:\n",
    "        The encoded form of the symbol.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(expression, L):\n",
    "        raise TypeError(\"Expected argument to subclass `L`\", expression)\n",
    "\n",
    "    # Base case\n",
    "    if isinstance(expression, Atomic):\n",
    "        name = expression.name\n",
    "        name_idx = atomic_symbols.index(name)\n",
    "        return codebook[name_idx]\n",
    "    # Inductive cases\n",
    "    elif isinstance(expression, Tuple):\n",
    "        lhs = expression.lhs\n",
    "        rhs = expression.rhs\n",
    "        return encode(lhs, atomic_symbols, codebook).bind(\n",
    "            encode(rhs, atomic_symbols, codebook)\n",
    "        )\n",
    "    elif isinstance(expression, Disjunction):\n",
    "        lhs = expression.lhs\n",
    "        rhs = expression.rhs\n",
    "        return encode(lhs, atomic_symbols, codebook) + encode(\n",
    "            rhs, atomic_symbols, codebook\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out our definitions to see if they maintain the same properties\n",
    "that we expect: namely,\n",
    "\n",
    "1. Atomic symbols should be self-similar, but very dissimlar to other atomic\n",
    "symbols.\n",
    "\n",
    "2. We should be able to retrieve the component parts of tuples.\n",
    "\n",
    "3. And, disjunctions should be similar to both component parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = (3, 400)\n",
      "enc_apple.cosine_similarity(enc_apple) = 1.0000000000000002\n",
      "enc_apple.cosine_similarity(enc_straw) = 0.00039153428747147444\n"
     ]
    }
   ],
   "source": [
    "dim = 400\n",
    "\n",
    "apple = Atomic(\"apple\")\n",
    "strawberry = Atomic(\"strawberry\")\n",
    "banana = Atomic(\"banana\")\n",
    "\n",
    "atomic_names = [\"apple\", \"strawberry\", \"banana\"]\n",
    "X = create_codebook(atomic_names, dim)\n",
    "print(f\"{X.shape = }\")\n",
    "\n",
    "enc_apple = encode(apple, atomic_names, X)\n",
    "enc_straw = encode(strawberry, atomic_names, X)\n",
    "enc_banana = encode(banana, atomic_names, X)\n",
    "\n",
    "print(f\"{enc_apple.cosine_similarity(enc_apple) = }\")\n",
    "print(f\"{enc_apple.cosine_similarity(enc_straw) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple similarity with encoded_apple: 0.09\n",
      "Tuple similarity with encoded_banana: 0.03\n"
     ]
    }
   ],
   "source": [
    "t = Tuple(banana, apple)\n",
    "enc_t = encode(t, atomic_names, X)\n",
    "\n",
    "print(f\"Tuple similarity with encoded_apple: {enc_t.cosine_similarity(enc_apple):.2f}\")\n",
    "print(\n",
    "    f\"Tuple similarity with encoded_banana: {enc_t.cosine_similarity(enc_banana):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disjunction similarity with encoded_apple: 0.72\n",
      "Disjunction similarity with encoded_banana: 0.72\n"
     ]
    }
   ],
   "source": [
    "d = Disjunction(banana, apple)\n",
    "enc_d = encode(d, atomic_names, X)\n",
    "\n",
    "print(\n",
    "    f\"Disjunction similarity with encoded_apple: {enc_d.cosine_similarity(enc_apple):.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Disjunction similarity with encoded_banana: {enc_d.cosine_similarity(enc_banana):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this section we introduced Vector-Symbolic Architectures or Hyperdimensional\n",
    "Computing: what kinds of operations they use, and the motivation behind them.\n",
    "We also discussed some basics about representing the abstract syntax of a \n",
    "language. In the next section, we will be investigating the more general question:\n",
    "given language $\\mathcal{L}$, how do we define a one-to-one mapping from $\\mathcal{L}$\n",
    "to $\\mathbb{R}^D$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | export\n",
    "class HRR(np.ndarray):\n",
    "    \"\"\"Thin wrapper around `np.ndarray` for Holographic Reduced Representations\n",
    "    (HRRs).\n",
    "    \"\"\"\n",
    "\n",
    "    # Incantation needed for subclassing `np.ndarray`.\n",
    "    def __new__(cls, input_array) -> \"HRR\":\n",
    "        obj = np.asarray(input_array).view(cls)\n",
    "        return obj\n",
    "\n",
    "    # Same as above\n",
    "    def __array_finalize__(self, obj: object) -> None:\n",
    "        if obj is None:\n",
    "            return\n",
    "\n",
    "    # The binding operation.\n",
    "    def bind(self, other: typing.Union[np.ndarray, \"HRR\"]) -> \"HRR\":\n",
    "        \"\"\"Perform circular convolution.\n",
    "\n",
    "        Args:\n",
    "            other (np.ndarray): Second argument.\n",
    "\n",
    "        Returns:\n",
    "            Circular convolution of the vector and the other.\n",
    "        \"\"\"\n",
    "        return ifft(fft(self) * fft(other)).real.view(HRR)\n",
    "\n",
    "    def inverse(self) -> \"HRR\":\n",
    "        \"\"\"Invert the vector for unbinding.\"\"\"\n",
    "        return self[np.r_[0, self.size - 1 : 0 : -1]].view(HRR)\n",
    "\n",
    "    def cosine_similarity(self, other: typing.Union[np.ndarray, \"HRR\"]) -> float:\n",
    "        return float((self.dot(other)) / (np.linalg.norm(self) * np.linalg.norm(other)))\n",
    "\n",
    "\n",
    "# | export\n",
    "def random(\n",
    "    num_vectors: int,\n",
    "    dim: int,\n",
    "    dtype: npt.DTypeLike = float,\n",
    "    rng=np.random.default_rng(),\n",
    ") -> \"HRR\":\n",
    "    r\"\"\"Create matrix of `n` `d`-dimensional HRR vectors, sampled from the normal\n",
    "    distribution,\n",
    "    $$\n",
    "    \\mathcal{N}(\\mu=1, \\sigma^2 = \\frac{1}{d}),\n",
    "    $$\n",
    "\n",
    "    Args:\n",
    "        num_vectors int: The number of vector symbols you wish to generate.\n",
    "        dim int: The dimension of the vector symbols.\n",
    "        dtype npt.DTypeLike: The `dtype` of the vector generated. Default: ``float``.\n",
    "        rng: Random number gene1ator.\n",
    "\n",
    "    Returns:\n",
    "        A ``(num_vectors, dim)`` matrix of random vector symbols.\n",
    "    \"\"\"\n",
    "    sd = 1.0 / np.sqrt(dim)\n",
    "    vs = rng.normal(scale=sd, size=(num_vectors, dim)).astype(dtype)\n",
    "    norms = np.linalg.vector_norm(vs, axis=1, keepdims=True)\n",
    "    vs /= norms\n",
    "    return HRR.__new__(cls=HRR, input_array=vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
